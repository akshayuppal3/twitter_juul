{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy \n",
    "import os\n",
    "import git\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "# import gensim\n",
    "# from gensim import corpora\n",
    "import os\n",
    "import networkx as nx\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Average\n",
    "from keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the labelled files and the poly_user\n",
    "def get_git_root(path):\n",
    "\tgit_repo = git.Repo(path, search_parent_directories=True)\n",
    "\tgit_root = git_repo.git.rev_parse(\"--show-toplevel\")\n",
    "\treturn git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup env\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize the text..\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tknzr = TweetTokenizer()\n",
    "def get_tokens(sentence):\n",
    "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
    "    tokens = tweet_tknzr.tokenize(sentence)\n",
    "    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1)]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'(https?://\\S+)', \"\", text) ## remove url\n",
    "    text = re.sub(r'(\\@\\w+)', \"author\",text)   ## remove @ mentions with author\n",
    "    text = re.sub(r'(@)', \"\",text)             ## remove @ symbols\n",
    "    text = re.sub(r'(author)',\"\",text)         ## remove author\n",
    "    text = re.sub(r'(#)', \"\",text)             ## removing the hashtags signal \n",
    "    text = re.sub(r'(RT )', \"\",text)         ## remove the retweet info as they dont convey any information\n",
    "    text.rstrip \n",
    "    text.lstrip\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the word2vec dict from the dictionary\n",
    "def get_word2vec(file_path):\n",
    "    file = open(file_path, \"r\")\n",
    "    if (file):\n",
    "        word2vec = dict()\n",
    "#         split = file.read().splitlines()\n",
    "        for line in file:\n",
    "            split_line = line.split(' ')\n",
    "            key = split_line[0] # the first word is the key\n",
    "            value = np.array([float(val) for val in split_line[1:]])\n",
    "            word2vec[key] = value\n",
    "        return (word2vec)\n",
    "    else:\n",
    "        print(\"invalid fiel path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the mean of the embeddings\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "\tdef __init__(self, word2vec):\n",
    "\t\tself.word2vec = word2vec\n",
    "\t\t# if a text is empty we should return a vector of zeros\n",
    "\t\t# with the same dimensionality as all the other vectors\n",
    "\t\tself.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\treturn np.array([\n",
    "\t\t\tnp.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "\t\t\t\t\tor [np.zeros(self.dim)], axis=0)\n",
    "\t\t\tfor words in tqdm(X)\n",
    "\t\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dir = os.path.join(get_git_root(os.getcwd()))\n",
    "input_dir = os.path.join(get_git_root(os.getcwd()),\"input\")\n",
    "embeddings_dir  = os.path.join(get_git_root(os.getcwd()),\"input\",\"embeddings\")\n",
    "annotatted_dir = os.path.join(input_dir,\"annotated_data\")\n",
    "classifier_dir = os.path.join(get_git_root(os.getcwd()),\"models\",\"classifier\")\n",
    "model_dir = os.path.join(get_git_root(os.getcwd()),\"models\")\n",
    "poly_dir = os.path.join(model_dir,\"poly_users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = os.path.join(embeddings_dir,\"glove.twitter.27B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_following_final = pd.read_csv(os.path.join(input_dir,\"following_final.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hexagon = pd.read_csv(os.path.join(input_dir,\"hexagonData.csv\"),lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_hexagon.userID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the input consolidated data (combing timeline and hexagon data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(os.path.join(input_dir,\"extraction\",\"consolidated_data.csv\"), lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation =pd.read_excel(os.path.join(annotatted_dir,\"annotated_new.xlsx\"),index_col=0) ## changed annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0 -news\n",
    "## 1 - promo\n",
    "## 2 -regular users\n",
    "print(df_annotation.label.value_counts())\n",
    "df_annotation = df_annotation.sample(frac=1)\n",
    "df_annotation = df_annotation.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take a balnced sample of 200 from each category\n",
    "df_news = (df_annotation[df_annotation.label == 0])[:200] \n",
    "df_reg = (df_annotation[df_annotation.label == 2])[:200] \n",
    "df_promo = (df_annotation[df_annotation.label == 1])[:200] \n",
    "df_annotation = pd.concat([df_news,df_promo,df_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random sample annotation file\n",
    "df_annotation = df_annotation.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize the sentences\n",
    "df_annotation['tweetText'] = df_annotation['tweetText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_annotation.loc[df_annotation.label.isin([1,2])].head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## looking at the tokenized data\n",
    "df_test = pd.DataFrame()\n",
    "df_test[\"tweetText\"] = list(df_annotation['tweetText'].apply(get_tokens))\n",
    "df_test['label'] = list(df_annotation[\"label\"])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting mean , quantile length of text\n",
    "df_len = list()\n",
    "for tokens in token_list:\n",
    "    df_len.append(len(tokens))\n",
    "df_len = pd.DataFrame(df_len)\n",
    "df_len.boxplot()\n",
    "df_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = int(df_len.quantile(0.95))  ## take the max_len = 90 percentile\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting keras tokenizer on annotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparign the keras tokenizer -- fitting on annnotation data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "keras_tkzr = Tokenizer()\n",
    "keras_tkzr.fit_on_texts(df_annotation[\"tweetText\"])\n",
    "vocab_size = len(keras_tkzr.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DONT RUN AGAIN!!\n",
    "word2vec = get_word2vec(os.path.join(embeddings_dir,glove_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting promoter, regular, news from data using bilstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the embedding matrix weights:\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def get_embedding_matrix(keras_tkzr,word2vec):\n",
    "    vocab_size = len(keras_tkzr.word_index) + 1\n",
    "    embedding_matrix = zeros((vocab_size, 100))\n",
    "    for word, i in keras_tkzr.word_index.items():\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return (embedding_matrix,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pass to the bi-lstm model\n",
    "def create_model(max_len,vocab_size,embedding_matrix):\n",
    "    input = Input(shape=(max_len,))\n",
    "    model = Embedding(vocab_size,100,weights=[embedding_matrix],input_length=max_len)(input)\n",
    "    model =  Bidirectional (LSTM (100,return_sequences=True,dropout=0.50),merge_mode='concat')(model)\n",
    "    model = TimeDistributed(Dense(100,activation='relu'))(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(100,activation='relu')(model)\n",
    "    output = Dense(3,activation='softmax')(model)\n",
    "    model = Model(input,output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "def load_data(df,max_len):\n",
    "    keras_tkzr = Tokenizer()\n",
    "    le = LabelEncoder()\n",
    "    keras_tkzr.fit_on_texts(df[\"tweetText\"])\n",
    "    ## getting input X\n",
    "    token_list = list(df['tweetText'].apply(get_tokens))\n",
    "    encoded_docs = keras_tkzr.texts_to_sequences(token_list)\n",
    "    max_len = max_len\n",
    "    X = pad_sequences(encoded_docs, maxlen=max_len, padding='post')\n",
    "    ## getting output Y\n",
    "    Y = (list(df['label']))\n",
    "    le.fit(Y)\n",
    "    le.classes_\n",
    "    y = le.transform(Y)\n",
    "    return (X,y,keras_tkzr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the trained model\n",
    "def train_evaluate_model(model,X_train,Y_train,X_test,Y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_pred = np.array([np.argmax(pred) for pred in Y_pred])\n",
    "    # scores = cross_val_score(model, X_test, Y_test, cv=5)\n",
    "    print('  Classification Report:\\n',classification_report(Y_test,y_pred),'\\n')\n",
    "    precision,recall,fscore,_ = precision_recall_fscore_support(Y_test,y_pred)\n",
    "    return (model,precision,recall,fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model,X_train,Y_train,epoch):\n",
    "    hist = model.fit(X_train,Y_train,validation_split=0.25, nb_epoch = epoch, verbose = 2)\n",
    "    return model,hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_folds = 3\n",
    "\n",
    "X, y, keras_tkzr = load_data(df_annotation,max_len)\n",
    "embedding_matrix, vocab_size = get_embedding_matrix(keras_tkzr,word2vec)\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "scores = []\n",
    "for i ,(train, test) in enumerate(skf.split(X,y)):\n",
    "    print (\"Running Fold\", i+ 1, \"/\", n_folds)\n",
    "    model = None # Clearing the NN.\n",
    "    model = create_model(max_len,vocab_size,embedding_matrix)\n",
    "    model,_ = fit_model(model, X[train], y[train],5)\n",
    "    model,precision,recall,fscore = train_evaluate_model(model, X[train], y[train], X[test], y[test])\n",
    "    scores.append([precision,recall,fscore])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = 27 ## bi-lstm \n",
    "f1_score = pd.DataFrame(fscore)\n",
    "print(f1_score.describe())\n",
    "f1_score.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "max_len = max_len\n",
    "X, y,keras_tkzr  = load_data(df_annotation,max_len)\n",
    "embedding_matrix, vocab_size = get_embedding_matrix(keras_tkzr,word2vec)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "model = create_model(max_len,vocab_size,embedding_matrix)\n",
    "model,hist = fit_model(model, X_train, Y_train,5)\n",
    "_, accuracy = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.array([np.argmax(pred) for pred in Y_pred])\n",
    "print('  Classification Report:\\n',classification_report(Y_test,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting baselines of tf-idf using SVM etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_tfidf(df):\n",
    "    tf_idf = TfidfVectorizer(sublinear_tf=True)\n",
    "    tf_idf.fit(df[\"tweetText\"])\n",
    "    feature_names = np.array(tf_idf.get_feature_names())\n",
    "    print(len(feature_names))\n",
    "    X = tf_idf.fit_transform(df[\"tweetText\"])\n",
    "    Y = (list(df['label']))\n",
    "    le = LabelEncoder()\n",
    "    le.fit(Y)\n",
    "    print(le.classes_)\n",
    "    y = le.transform(Y)\n",
    "    print(\"X.shape:\",X.shape)\n",
    "    return (X,y,tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_wrapper(X_train,Y_train):\n",
    "    param_grid = [\n",
    "    {'C': [1, 10], 'kernel': ['linear']},\n",
    "    {'C': [1, 10], 'gamma': [0.1,0.01], 'kernel': ['rbf']},]\n",
    "    svm1 = GridSearchCV(SVC(),param_grid)\n",
    "    svm1.fit(X_train, Y_train)\n",
    "    return(svm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_annotation.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y, tfidf = load_data_tfidf(df_annotation)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "# sv = svm_wrapper(X_train,Y_train)\n",
    "svm_tf = SVC(kernel='linear',C=1,)#gamma=0.1)\n",
    "svm_tf.fit(X_train,Y_train)\n",
    "scores = cross_val_score(svm, X,y, cv=10)\n",
    "Y_pred = svm_tf.predict(X_test)\n",
    "print('  Classification Report SVM:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "## accuarcy score\n",
    "(Y_test == Y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dump the svm as the best model\n",
    "path_classifier = os.path.join(classifier_dir,\"filter\",\"svm_tf.pkl\")\n",
    "path_vectorizer = os.path.join(classifier_dir,\"filter\",\"tf_idf_vect.pkl\")\n",
    "with open(path_classifier,\"wb\") as f:\n",
    "    pickle.dump(svm_tf,f)\n",
    "with open(path_vectorizer,\"wb\") as f:\n",
    "    pickle.dump(tfidf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "X,y, tfidf = load_data_tfidf(df_annotation)\n",
    "cross_scores = (cross_val_score(svm_tf, X, y, cv=10))  \n",
    "np.mean(cross_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "rf.fit(X_train,Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "print('  Classification Report rf:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "## accuarcy score\n",
    "(Y_test == Y_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baselines with embedding using SVM, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vect = MeanEmbeddingVectorizer(word2vec)\n",
    "token_list = list(df_annotation['tweetText'].apply(get_tokens))\n",
    "X = vect.transform(token_list)\n",
    "y = np.array(list(df_annotation[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sv = svm_wrapper(X_train,Y_train)\n",
    "svm = SVC(kernel='rbf',C=1,gamma=0.1)\n",
    "svm.fit(X_train,Y_train)\n",
    "scores = cross_val_score(svm, X,y, cv=10)\n",
    "Y_pred = svm.predict(X_test)\n",
    "print('  Classification Report SVM:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "## accuarcy score\n",
    "(Y_test == Y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "rf.fit(X_train,Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "print('  Classification Report SVM:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "## accuarcy score\n",
    "(Y_test == Y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree = ExtraTreesClassifier(n_estimators=200)\n",
    "scores = cross_val_score(etree, X,y, cv=10)\n",
    "etree.fit(X_train,Y_train)\n",
    "Y_pred = etree.predict(X_test)\n",
    "print('  Classification Report extra tree:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "## accuarcy score\n",
    "(Y_test == Y_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting promoter, user, news \n",
    "#### using with the svm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(os.path.join(input_dir,\"extraction\",\"consolidated_data.csv\"), lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model and tokenizer\n",
    "svm_tf = pickle.load(open(os.path.join(classifier_dir,\"filter\",\"svm_tf.pkl\"),\"rb\"))\n",
    "tfidf_vect = pickle.load(open(os.path.join(classifier_dir,\"filter\",\"tf_idf_vect.pkl\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_tweets[\"tweetText\"]\n",
    "X = tfidf_vect.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[\"label\"] = svm_tf.predict(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regualr users\n",
    "use= len(df_tweets.loc[df_tweets.label == 2])\n",
    "print(use/len(df_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## news campaign\n",
    "news = len(df_tweets.loc[df_tweets.label == 0])\n",
    "print(news/ len(df_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## promotional tweets\n",
    "prom = len(df_tweets.loc[df_tweets.label == 1])\n",
    "print(prom/ len(df_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot of user, promoter and news.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_total = df_tweets.userID.unique()\n",
    "print(\"total users\", len(users_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## capture each category % for each user\n",
    "tweets_type = list()\n",
    "for idx,user in enumerate(tqdm(users_total)):\n",
    "    user_tweets = df_tweets.loc[df_tweets.userID == user] ## selecting tweets of that user\n",
    "    news = user_tweets.loc[user_tweets.label == 0]\n",
    "    prom = user_tweets.loc[user_tweets.label == 1]\n",
    "    uexp = user_tweets.loc[user_tweets.label == 2]\n",
    "    temp = {\"userID\": user, \n",
    "             \"prom\" : (len(prom)/ len(user_tweets)) * 100,\n",
    "             \"news\" : (len(news)/ len(user_tweets)) * 100,\n",
    "             \"user_express\": (len(uexp) / len(user_tweets)) * 100,\n",
    "           }\n",
    "    tweets_type.append(temp)\n",
    "tweets_type = pd.DataFrame(tweets_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_type = tweets_type.sort_values(by=[\"user_express\",\"news\",\"prom\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [str(user) for user in list(tweets_type.userID)]\n",
    "uexp = list(tweets_type[\"user_express\"])\n",
    "news = list(tweets_type[\"prom\"])\n",
    "prom = list(tweets_type[\"news\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "p1 = plt.bar(users, uexp, color='b',label= \"users\")\n",
    "p2 = plt.bar(users, news, bottom=np.array(uexp), color='g',label=\"news\")\n",
    "p3 = plt.bar(users, prom, \n",
    "             bottom=np.array(uexp)+np.array(news), color='r', label= \"promoter\")\n",
    "# p4 = plt.bar(ind, dataset[4], width,\n",
    "#              bottom=np.array(dataset[1])+np.array(dataset[2])+np.array(dataset[3]),\n",
    "#              color='c')\n",
    "plt.legend(fontsize= 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting regular, promoters and news users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predicting users based on the category of\n",
    "category = list()\n",
    "for idx,tweet in tweets_type.iterrows():\n",
    "    cat = np.argmax(list([tweet[\"news\"],tweet[\"prom\"],tweet[\"user_express\"]]))\n",
    "    category.append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"promoters\",len([ele for ele in category if ele == 0]))\n",
    "print(\"news users\", len([ele for ele in category if ele == 1]))\n",
    "print(\"normal users\", len([ele for ele in category if ele == 2]))\n",
    "normal_users = [ele for ele in category if ele == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the all different no of tweets as a dataframe for viz and future purposes -- dumped\n",
    "def get_tweet_distr(df,column) -> dict():\n",
    "    i = 0\n",
    "    news_t = dict()\n",
    "    news = np.sort(list(df[column]))\n",
    "    news_t[0] = 0\n",
    "    for idx,value in enumerate(news):\n",
    "        if (value > 30):\n",
    "            if (value < (i+50) ):\n",
    "                news_t[i] += 1\n",
    "            else:\n",
    "                i = i + 50\n",
    "                news_t[i] = 1\n",
    "        if(i > 350):\n",
    "            break\n",
    "    return(news_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @params dataframe and returns max category of user\n",
    "def get_category(df) -> int:\n",
    "    return max(df[\"news\"],df[\"prom\"],df[\"user_express\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ploting the news tweets distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "news_t = get_tweet_distr(tweets_type,\"news\")\n",
    "prom_t = get_tweet_distr(tweets_type, \"prom\")\n",
    "euser_t = get_tweet_distr(tweets_type, \"user_express\") \n",
    "# ax = plt.subplot(111)\n",
    "plt.bar(np.arange(0,len(news_t.keys())*50,50) - 20, news_t.values(), width = 20, label = \"news\",align='center')\n",
    "plt.bar(np.arange(0,len(news_t.keys())*50,50), prom_t.values() , width = 20, label= \"promotion\", align='center')\n",
    "plt.bar(np.arange(0,len(news_t.keys())*50,50) + 20, euser_t.values(), width = 20, label = \"user_expressed\", align='center')\n",
    "plt.xticks(np.arange(0,650,50))\n",
    "plt.ylabel(\"number of users\")\n",
    "plt.xlabel(\"number of tweets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predicting users based on the category of\n",
    "category = list()\n",
    "for idx,tweet in tweets_type.iterrows():\n",
    "    cat = np.argmax(list([tweet[\"news\"],tweet[\"prom\"],tweet[\"user_express\"]]))\n",
    "    category.append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_type[\"category\"] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dump labelled tweets and tweets type\n",
    "# file_name = os.path.join(input_dir,\"labelled_data\",\"tweets_predicted.csv\")\n",
    "# df_tweets.to_csv(file_name,index=False)\n",
    "# file_name = os.path.join(input_dir,\"labelled_data\",\"users_type.csv\")\n",
    "# tweets_type.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the normal users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting all the tweets from normal users(who have max number of user expressed tweets)\n",
    "regular_users = list(tweets_type[\"userID\"].loc[tweets_type.category == 2])\n",
    "normal_tweets = (df_tweets.loc[df_tweets.userID.isin(normal_users)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(regular_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normal_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting poly and mono users.. (check with most common words as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get poly and mono users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokens_list):\n",
    "    vocab = Counter()\n",
    "    for tokens in tokens_list:\n",
    "        vocab.update(tokens)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return clean text\n",
    "def prepare_text_LDA(sentence):\n",
    "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
    "    tweet_tknzr = TweetTokenizer()\n",
    "    tokens = tweet_tknzr.tokenize(sentence)\n",
    "    tokens = [token.lower() for token in tokens if (token not in stopwords and token.isalpha())] # stopwords removal\n",
    "    tokens = [get_lemma(token) for token in tokens]  # lemmatization\n",
    "    return (\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join al the of the tweets for each user\n",
    "def get_tweets_user(df):\n",
    "    df_tweets_comb = list()\n",
    "    users = df.userID.unique()\n",
    "    for user in users:\n",
    "        tweets =(df.loc[df.userID == user])\n",
    "        tweets_f = (\" . \".join(tweets[\"tweetText\"]))\n",
    "        df_tweets_comb.append((user,tweets_f))\n",
    "    df_user = pd.DataFrame(df_tweets_comb, columns=[\"userID\",\"tweetText\"])\n",
    "    return (df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting the pattern for weeds\n",
      "extracting the pattern for juul\n"
     ]
    }
   ],
   "source": [
    "## getting the weed and juul pattern\n",
    "print(\"extracting the pattern for weeds\")\n",
    "weed_words = pickle.load(open(os.path.join(model_dir, \"weed_words.pkl\"), \"rb\"))\n",
    "weed_words.remove('CBD oil')\n",
    "# weed_words = [(\" \" + word + \" \") for word in weed_words]\n",
    "pattern_weed = \"|\".join(weed_words)\n",
    "print(\"extracting the pattern for juul\")\n",
    "pattern_juul = 'juul'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_weed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocesing the tweets\n",
    "normal_tweets[\"tweetText\"] = normal_tweets[\"tweetText\"].apply(clean_text)\n",
    "normal_tweets[\"tweetText\"] = (normal_tweets[\"tweetText\"].apply(prepare_text_LDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the weed tweets\n",
    "weed_tweets = normal_tweets[normal_tweets['tweetText'].str.contains(pattern_weed, case=False)]\n",
    "print(\"weed tweets = \",len(weed_tweets))\n",
    "poly_users = list(weed_tweets.userID.unique())\n",
    "print(\"no of poly users = \", len(poly_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_users = list(normal_tweets.userID.unique())\n",
    "poly_length = len(poly_users)\n",
    "total_users_length = len(normal_users)\n",
    "mono_length = total_users_length - poly_length\n",
    "print(\"total users = \", total_users_length)\n",
    "print(\"no of poly users = \", poly_length)\n",
    "print(\"no of mono users = \", mono_length)\n",
    "print(\"% of poly users is \", poly_length / total_users_length)\n",
    "print(\"% of mono users is \", mono_length / total_users_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dump the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the poly_users,regualar_users\n",
    "# poly_dir = os.path.join(model_dir,\"poly_users\")\n",
    "# pathname = os.path.join(poly_dir,\"poly_users.pkl\")\n",
    "# dump_obj(poly_users,pathname)\n",
    "# pathname = os.path.join(poly_dir,\"normal_users.pkl\")\n",
    "# dump_obj(regular_users,pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def dump_obj(obj,pathname):\n",
    "    with open(pathname,\"wb\") as f:\n",
    "        pickle.dump(obj,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the poly - mono users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_users = pickle.load(open(os.path.join(poly_dir,\"poly_users.pkl\"),\"rb\"))\n",
    "normal_users = pickle.load(open(os.path.join(poly_dir,\"normal_users.pkl\"),\"rb\"))\n",
    "df_tweets = pd.read_csv(os.path.join(input_dir,\"labelled_data\",\"tweets_predicted.csv\"),lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_tweets = df_tweets.loc[df_tweets.userID.isin(normal_users)]\n",
    "poly_tweets = df_tweets.loc[df_tweets.userID.isin(poly_users)]\n",
    "mono_users = list(set(normal_users) - set(poly_users))\n",
    "mono_tweets = df_tweets.loc[df_tweets.userID.isin(mono_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all tweets by poly users: \", len(poly_tweets))\n",
    "print(\"poly users (based with retweets):\", len(poly_tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all tweets by mono users: \", len(mono_tweets))\n",
    "print(\"mono users (based with retweets):\", len(mono_tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning the text\n",
    "poly_tweets[\"tweetText\"] = poly_tweets[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets[\"tweetText\"] = mono_tweets[\"tweetText\"].apply(clean_text)\n",
    "## tokenizing and lemmatizing\n",
    "poly_tweets[\"tweetText\"] = (poly_tweets[\"tweetText\"].apply(prepare_text_LDA))\n",
    "mono_tweets[\"tweetText\"] = mono_tweets[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting most coomon words for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_tweets_user = get_tweets_user(poly_tweets)\n",
    "mono_tweets_user = get_tweets_user(mono_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_p = list(poly_tweets_user[\"tweetText\"].apply(get_tokens))\n",
    "tokens_j = list(mono_tweets_user[\"tweetText\"].apply(get_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(tokens_p)\n",
    "vocab.most_common()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## most common words for juul user tweets\n",
    "vocab1 = build_vocab(tokens_j)\n",
    "vocab1.most_common()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = os.path.join(model_dir,\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokens_p)  # tokens_j\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens_p] # tokens_j\n",
    "pickle.dump(corpus, open(os.path.join(temp_dir,'corpus_m.pkl'), 'wb'))\n",
    "dictionary.save(os.path.join(temp_dir,'dictionary_m.gensim'))\n",
    "## LDA\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=8)\n",
    "ldamodel.save(os.path.join(temp_dir,'model_m.gensim'))\n",
    "topics = ldamodel.print_topics(num_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## poly\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(temp_dir,'dictionary_m.gensim'))\n",
    "corpus = pickle.load(open(os.path.join(temp_dir,'corpus_m.pkl'), 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load(os.path.join(temp_dir,'model_m.gensim'))\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### mono\n",
    "dictionary = corpora.Dictionary(tokens_j)  # tokens_j\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens_j] # tokens_j\n",
    "pickle.dump(corpus, open(os.path.join(temp_dir,'corpus_m.pkl'), 'wb'))\n",
    "dictionary.save(os.path.join(temp_dir,'dictionary_m.gensim'))\n",
    "## LDA\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=8)\n",
    "ldamodel.save(os.path.join(temp_dir,'model_m.gensim'))\n",
    "topics = ldamodel.print_topics(num_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mono\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(temp_dir,'dictionary_m.gensim'))\n",
    "corpus = pickle.load(open(os.path.join(temp_dir,'corpus_m.pkl'), 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load(os.path.join(temp_dir,'model_m.gensim'))\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training_classifier to predict poly vs mono user -- no longer valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get further categorization of poly_subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @return the posotion of juul timeline in the list of weeds\n",
    "def get_postion(times_j, list_w):\n",
    "    if list_w is not None:\n",
    "        pos = -1\n",
    "        for idx,ele in enumerate(list_w):\n",
    "            if (times_j) > ele:\n",
    "                continue\n",
    "            else:\n",
    "                pos = idx\n",
    "                break\n",
    "        if pos == -1:\n",
    "            pos = len(list_w)\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining hexagon_tweets with the hexagon_timeline tweets \n",
    "df_hexagon = pd.read_csv(os.path.join(input_dir,\"hexagonData.csv\"),lineterminator=\"\\n\")\n",
    "df_sub = df_hexagon.loc[df_hexagon.userID.isin(df_tweets.userID.unique())]\n",
    "df_final = pd.concat([df_tweets,df_sub])\n",
    "len(df_final.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** starting with the poly sub type users\")\n",
    "juul_before = list()\n",
    "juul_after = list()\n",
    "# poly_middle = list()\n",
    "_und = list()\n",
    "total_users = list(df_final.userID.unique())\n",
    "for user in tqdm(poly_users):\n",
    "    user_tweets = df_final.loc[df_final.userID == user]\n",
    "    user_tweets = user_tweets.sort_values(by='tweetCreatedAt', ascending=True)  # sort by tweet created at\n",
    "    times_j = None\n",
    "    list_w = None\n",
    "    j_anchor = list(user_tweets['tweetCreatedAt'].loc[user_tweets['tweetText'].str.contains(pattern_juul, case=False)]) ## get the first occurance\n",
    "    if j_anchor:\n",
    "        time_j = pd.to_datetime(j_anchor[0])\n",
    "    else:\n",
    "        time_j = None\n",
    "    weed_tweets = user_tweets[user_tweets['tweetText'].str.contains(pattern_weed, case=False)]\n",
    "    if (len(weed_tweets) > 0):\n",
    "        if (len(weed_tweets) > 0):\n",
    "            list_w = pd.to_datetime(list(weed_tweets[\n",
    "                                              'tweetCreatedAt']))\n",
    "    else:\n",
    "        list_w = None\n",
    "    if (time_j is not None and list_w is not None):\n",
    "        pos = get_postion(time_j,list_w)              ## @TODO change..create function\n",
    "        if (pos == 0):\n",
    "            juul_before.append(user)\n",
    "        else:\n",
    "            juul_after.append(user)\n",
    "    else:\n",
    "        _und.append(user)                         ## if we can't determine the juul tweets in the user..\n",
    "print(\"Poly type users calculated\")\n",
    "print(\"total poly users =\", len(poly_users))\n",
    "print(\"****************\\n\")\n",
    "print(\"% of juul before users = \", len(juul_before) / len(poly_users))\n",
    "print(\"len of juul before = \",len(juul_before))\n",
    "print(\"\\n\")\n",
    "print(\"% of juul after users = \", len(juul_after) / len(poly_users))\n",
    "print(\"len of juul after = \",len(juul_after))\n",
    "print(\"\\n\")\n",
    "print(\"len of undefined users = \", len(_und) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poly type users calculated\n",
    "total users = 640\n",
    "\n",
    "****************\n",
    "\n",
    "% of juul before users =  0.190625\n",
    "len of juul before =  122\n",
    "\n",
    "\n",
    "% of juul after users =  0.809375\n",
    "len of juul after =  518\n",
    "\n",
    "\n",
    "% of undefined users =  0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting the change of poly_before to poly again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the poly_before user juul before and first weed tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(juul_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juul_before_tweets = df_final.loc[df_final.userID.isin(juul_before)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(juul_before_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juul_tweets = juul_before_tweets.loc[juul_before_tweets[\"tweetText\"].str.contains(pattern_juul,case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all the juul before users contain juul tweets\n",
    "len(juul_tweets.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juul_tweets[\"tweetCreatedAt\"] = pd.to_datetime(juul_tweets[\"tweetCreatedAt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed = []\n",
    "for user in juul_before:\n",
    "    user_tweets = juul_tweets.loc[juul_tweets.userID == user]\n",
    "    user_tweets = user_tweets.sort_values(by=[\"tweetCreatedAt\"])\n",
    "    if (list(user_tweets[\"tweetCreatedAt\"])):\n",
    "        anchor_juul = list(user_tweets[\"tweetCreatedAt\"])[0]\n",
    "        poly_first_juul_weed.append([user,anchor_juul])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed = pd.DataFrame(poly_first_juul_weed,columns=[\"userID\",\"juul_first\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = juul_before_tweets.loc[juul_before_tweets[\"tweetText\"].str.contains(pattern_weed,case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_first = []\n",
    "for user in juul_before:\n",
    "    user_tweets = weed_tweets.loc[weed_tweets.userID == user]\n",
    "    user_tweets = user_tweets.sort_values(by=[\"tweetCreatedAt\"])\n",
    "    if (list(user_tweets[\"tweetCreatedAt\"])):\n",
    "        anchor_weed = list(user_tweets[\"tweetCreatedAt\"])[0]\n",
    "        anchor_weed = pd.to_datetime(anchor_weed)\n",
    "        weed_first.append(anchor_weed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed[\"weed_first\"] = weed_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed = poly_first_juul_weed.sort_values(by=[\"juul_first\"],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "users_list = list(poly_first_juul_weed.userID.unique())\n",
    "plt.scatter(list(poly_first_juul_weed[\"juul_first\"]),[str(user) for user in users_list],color=\"blue\")\n",
    "plt.scatter(list(poly_first_juul_weed[\"weed_first\"]), [str(user) for user in users_list],color=\"green\")\n",
    "plt.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    left=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelleft=False) # labels along the bottom edge are of\n",
    "plt.ylabel(\"users\", fontsize = 15)\n",
    "plt.xlabel(\"timeline \", fontsize=15)\n",
    "plt.title(\"timeline of change of users from mono to poly\" )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed[\"interval\"] = poly_first_juul_weed[\"weed_first\"].dt.date - poly_first_juul_weed[\"juul_first\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [ele.days() for ele in list(poly_first_juul_weed[\"interval\"])]\n",
    "days.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "counts = {}\n",
    "delta = 5000\n",
    "counts[(pos+delta)] = 0\n",
    "for ele in days:\n",
    "    if  ele < pos + delta:\n",
    "        counts[(pos+ delta)] += 1\n",
    "    else:\n",
    "        pos += delta\n",
    "        counts[(pos + delta)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [ele[1] for ele in sorted(counts.items(), key=lambda x: x[0])]\n",
    "keys = [ele[0] for ele in sorted(counts.items(), key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.bar(keys,values,width = 2)\n",
    "plt.title(\"Delay between juul first and substance tweet\")\n",
    "plt.xlabel(\"Delay (days)\")\n",
    "plt.ylabel(\"Delay\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gettng the network properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_following_final = pd.read_csv(os.path.join(input_dir,\"following_final.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(normal_users))\n",
    "print(len(poly_users))\n",
    "print(len(mono_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## selecting a subset of the following that contain the total users\n",
    "df_poly_users = df_following_final[df_following_final.userID.isin(poly_users)]\n",
    "df_mono_users = df_following_final[df_following_final.userID.isin(mono_users)]\n",
    "df_all_users = df_following_final[df_following_final.userID.isin(total_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(df):\n",
    "    G = nx.DiGraph()\n",
    "    users = list(df.userID.unique())\n",
    "    for user in tqdm(users):\n",
    "        following_A = set(ast.literal_eval((df.loc[df.userID == user].head(1)[\"following\"].values)[0]))\n",
    "        user_set = set([node for node in users if node != user])\n",
    "        users_list = user_set.intersection(following_A)\n",
    "        for user_following in list(users_list):\n",
    "            G.add_edge(user,user_following)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_poly = get_graph(df_poly_users)\n",
    "G_mono = get_graph(df_mono_users)\n",
    "G_all = get_graph(df_all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color map \n",
    "def get_color_map(G,poly_users):\n",
    "    color_map = list()\n",
    "    for node in G.nodes():\n",
    "        if int(node) in set(poly_users):\n",
    "            color_map.append('green')\n",
    "        else:\n",
    "            color_map.append('blue')\n",
    "    return color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mono viz --(filetersd)\n",
    "color_map = get_color_map(G_mono,poly_users)\n",
    "plt.figure(figsize=(30,30))\n",
    "nx.draw(G_mono,node_color=color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = get_color_map(G_poly,poly_users)\n",
    "plt.figure(figsize=(30,30))\n",
    "nx.draw(G_poly,node_color=color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "color_map = get_color_map(G_all,poly_users)\n",
    "pos = nx.spring_layout(G_all)\n",
    "nx.draw_networkx(G_all, pos=pos, node_color = color_map,with_label = False,arrows=True,labels=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G_all))\n",
    "print(\"average clustering poly\",nx.average_clustering(G_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G_mono))\n",
    "print(\"average clustering poly\",nx.average_clustering(G_mono))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G_poly))\n",
    "print(\"average clustering mono\",nx.average_clustering(G_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the change of mono as(poly_before) to poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date_format = \"%Y-%m-%d\"\n",
    "time_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "my_date = datetime.datetime.strptime(\"2015-01-03\",date_format)\n",
    "final_date = datetime.datetime.combine(my_date,datetime.time(10,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_date.strftime(time_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bucket the users in bucket :\n",
    "#1 2014 - [2015-2018] \n",
    "#2 2014- 2015 - [16 - 18]\n",
    "#3 2014 - 2016 - [17-18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_hexagon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all = df_tweets.loc[df_tweets.userID.isin(normal_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_all.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first bucket \n",
    "tweets_all[\"tweetCreatedAt\"] = pd.to_datetime(tweets_all.tweetCreatedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2014 = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(tweets_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2014.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining all of the tweets of the user\n",
    "tweets_2014 = get_tweets_user(tweets_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for idx,row in tweets_2014.iterrows():\n",
    "    if row['userID'] in poly_users:\n",
    "        label.append(\"poly\")\n",
    "    else:\n",
    "        label.append(\"mono\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2014[\"label\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "tweets_2014[\"tweetText\"] = tweets_2014[\"tweetText\"].apply(clean_text)\n",
    "tweets_2014[\"tweetText\"] = tweets_2014[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load the model and tokenizer\n",
    "lstm_model = pickle.load(open(os.path.join(poly_dir,\"pomo_classifier_lstm.pkl\"),\"rb\"))\n",
    "keras_tkzr = pickle.load(open(os.path.join(poly_dir,\"keras_tkzr.pkl\"),\"rb\"))\n",
    "le = pickle.load(open(os.path.join(poly_dir,\"label_encoder.pkl\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(tweets_2014[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in lstm_model.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Y_true == Y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taking only mono users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the keywords to gather all of the mono users out of the poly ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"extracting the pattern for weeds\")\n",
    "weed_words = pickle.load(open(os.path.join(model_dir, \"weed_words.pkl\"), \"rb\"))\n",
    "pattern_weed = \"|\".join(weed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_weed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = tweets_2014[tweets_2014[\"tweetText\"].str.contains(pattern_weed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = tweets_2014.userID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_user_a = weed_tweets.userID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poly_user_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_users_a = list(set(total_users)  - set(set(poly_user_a))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mono_users_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_tweets_2014 = tweets_2014.loc[tweets_2014.userID.isin(mono_users_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mono_tweets_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for idx,row in mono_tweets_2014.iterrows():\n",
    "    if row['userID'] in poly_users:\n",
    "        label.append(\"poly\")\n",
    "    else:\n",
    "        label.append(\"mono\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model and tokenizer\n",
    "pomo_lstm = pickle.load(open(os.path.join(poly_dir,\"pomo_classifier_lstm.pkl\"),\"rb\"))\n",
    "keras_tkzr = pickle.load(open(os.path.join(poly_dir,\"keras_tkzr.pkl\"),\"rb\"))\n",
    "le = pickle.load(open(os.path.join(poly_dir,\"label_encoder.pkl\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "mono_tweets_2014[\"tweetText\"] = mono_tweets_2014[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets_2014[\"tweetText\"] = mono_tweets_2014[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ele for ele in Y_true if ele == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(mono_tweets_2014[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in pomo_lstm.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels: \n",
    "mono 0\n",
    "poly 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2015 = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = (tweets_2015.userID.unique())\n",
    "len(total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining all of the tweets of the user\n",
    "tweets_2015 = get_tweets_user(tweets_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = tweets_2015[tweets_2015[\"tweetText\"].str.contains(pattern_weed)]\n",
    "len(weed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_user_a = (weed_tweets.userID.unique())\n",
    "len(poly_user_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_users_a = list(set(total_users)  - set(set(poly_user_a))) \n",
    "len(mono_users_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_tweets_2015 = tweets_2015[tweets_2015.userID.isin(mono_users_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df,poly_users):\n",
    "    label = []\n",
    "    for idx,row in df.iterrows():\n",
    "        if row['userID'] in poly_users:\n",
    "            label.append(\"poly\")\n",
    "        else:\n",
    "            label.append(\"mono\")\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(mono_tweets_2015,poly_users)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model and tokenizer\n",
    "pomo_lstm = pickle.load(open(os.path.join(poly_dir,\"pomo_classifier_lstm.pkl\"),\"rb\"))\n",
    "keras_tkzr = pickle.load(open(os.path.join(poly_dir,\"keras_tkzr.pkl\"),\"rb\"))\n",
    "le = pickle.load(open(os.path.join(poly_dir,\"label_encoder.pkl\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "mono_tweets_2015[\"tweetText\"] = mono_tweets_2015[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets_2015[\"tweetText\"] = mono_tweets_2015[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mono\",len([ele for ele in Y_true if ele == 0]))\n",
    "print(\"poly\",len([ele for ele in Y_true if ele == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(mono_tweets_2015[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in pomo_lstm.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bucket3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2016 = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = (tweets_2016.userID.unique())\n",
    "len(total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining all of the tweets of the user\n",
    "tweets_2016 = get_tweets_user(tweets_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = tweets_2016[tweets_2016[\"tweetText\"].str.contains(pattern_weed)]\n",
    "len(weed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_user_a = (weed_tweets.userID.unique())\n",
    "len(poly_user_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_users_a = list(set(total_users)  - set(set(poly_user_a))) \n",
    "len(mono_users_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_tweets_2016 = tweets_2016[tweets_2016.userID.isin(mono_users_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(mono_tweets_2016,poly_users)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "mono_tweets_2016[\"tweetText\"] = mono_tweets_2016[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets_2016[\"tweetText\"] = mono_tweets_2016[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mono\",len([ele for ele in Y_true if ele == 0]))\n",
    "print(\"poly\",len([ele for ele in Y_true if ele == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(mono_tweets_2016[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in pomo_lstm.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bucket 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2017 = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = (tweets_2017.userID.unique())\n",
    "len(total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining all of the tweets of the user\n",
    "tweets_2017 = get_tweets_user(tweets_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = tweets_2017[tweets_2017[\"tweetText\"].str.contains(pattern_weed)]\n",
    "len(weed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_user_a = (weed_tweets.userID.unique())\n",
    "len(poly_user_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_users_a = list(set(total_users)  - set(set(poly_user_a))) \n",
    "len(mono_users_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_tweets_2017 = tweets_2017[tweets_2017.userID.isin(mono_users_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(mono_tweets_2017,poly_users)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "mono_tweets_2017[\"tweetText\"] = mono_tweets_2017[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets_2017[\"tweetText\"] = mono_tweets_2017[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mono\",len([ele for ele in Y_true if ele == 0]))\n",
    "print(\"poly\",len([ele for ele in Y_true if ele == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(mono_tweets_2017[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in pomo_lstm.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "ax.bar(x_label,f1_classifier,width=0.4)\n",
    "plt.title(\"Change of accuarcy of classifier to recongnize mono- > poly on different range of data\",fontsize=14)\n",
    "plt.xlabel(\"date range\",fontsize=12)\n",
    "plt.ylabel(\"f1 score of the classifier\",fontsize=12)\n",
    "for i, v in enumerate(f1_classifier):\n",
    "    plt.text(i - 0.09,v + 0.01, str(v), color='blue', fontweight='bold')\n",
    "#-- need to change with f1 score acuarcy instead of just poly_accuarcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking at the most predictive weights for the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer as keras_Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "keras_tkzr = keras_Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df):\n",
    "    df[\"tweetText\"] = df[\"tweetText\"].apply(clean_text)\n",
    "    df[\"tweetText\"] = df[\"tweetText\"].apply(prepare_text_LDA)\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    # poly_users global\n",
    "    label = []\n",
    "    for idx,row in df.iterrows():\n",
    "        if row['userID'] in poly_users:\n",
    "            label.append(\"poly\")\n",
    "        else:\n",
    "            label.append(\"mono\")\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(label):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(label)\n",
    "    y = le.transform(label)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    label = get_label(df)\n",
    "#     y = encode(label)\n",
    "    tf_idf = TfidfVectorizer(sublinear_tf=True)\n",
    "    tf_idf.fit(df[\"tweetText\"])\n",
    "    feature_names = np.array(tf_idf.get_feature_names())\n",
    "    X = tf_idf.fit_transform(df[\"tweetText\"])\n",
    "    print(X.shape)\n",
    "    return (X,np.array(label),tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_wrapper(X_train,Y_train):\n",
    "    svm = LinearSVC(C=10)\n",
    "    svm.fit(X_train,Y_train,)\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coeff(k,model,feature_names):\n",
    "    coef = (model.coef_.ravel()) \n",
    "    top_positive_coefficients = np.argsort(coef)[-k:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:k]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    colors = ['red' if c < 0 else 'blue' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * k), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 1 + 2 * k), feature_names[top_coefficients], rotation=60, ha='right',fontsize= 20)\n",
    "    plt.show()\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all = df_tweets.loc[df_tweets.userID.isin(normal_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total tweets \",len(tweets_all))\n",
    "print(\"users\", len(tweets_all.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ = clean_tweets(tweets_all)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf_final = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm_final = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(Y_test == Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shape\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocalb size\n",
    "print(len(tf_idf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm_final,tf_idf_final.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "coef = (svm.coef_.ravel()) \n",
    "feature_names = np.array(tf_idf.get_feature_names())\n",
    "top_positive_coefficients = np.argsort(coef)[-k:]\n",
    "list(feature_names[top_positive_coefficients][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_negative_coefficients = np.argsort(coef)[:k]\n",
    "list(feature_names[top_negative_coefficients])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buckets - svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bucket 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all = df_tweets.loc[df_tweets.userID.isin(normal_users)]\n",
    "f_scores = []\n",
    "years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all[\"tweetCreatedAt\"] = pd.to_datetime(tweets_all.tweetCreatedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"tweets\",len(tweets))\n",
    "print(\"users\",len(tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "_,_,fscore,_ = precision_recall_fscore_support(Y_test,Y_pred)\n",
    "# print(np.average(fscore))\n",
    "f_scores.append(np.average(fscore)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm,tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "years = [\"2014-2015\"]\n",
    "plt.scatter(years, f_scores)\n",
    "plt.title(\"Training accuarcy of classifier(SVM) on tweets for different time range\",fontsize = 20)\n",
    "plt.xlabel(\"Range of year span \",fontsize = 16)\n",
    "plt.ylabel(\"f1-score\",fontsize = 16)\n",
    "plt.xticks(rotation=30, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bucket 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"tweets\",len(tweets))\n",
    "print(\"users\",len(tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "_,_,fscore,_ = precision_recall_fscore_support(Y_test,Y_pred)\n",
    "# print(np.average(fscore))\n",
    "f_scores.append(np.average(fscore)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm,tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "years = [\"2014-15\",\"2014-16\"]\n",
    "plt.scatter(years, f_scores)\n",
    "plt.title(\"Training accuarcy of classifier(SVM) on tweets for different time range\",fontsize = 20)\n",
    "plt.xlabel(\"Range of year span \",fontsize = 16)\n",
    "plt.ylabel(\"f1-score\",fontsize = 16)\n",
    "plt.xticks(rotation=30, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"tweets\",len(tweets))\n",
    "print(\"users\",len(tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "_,_,fscore,_ = precision_recall_fscore_support(Y_test,Y_pred)\n",
    "# print(np.average(fscore))\n",
    "f_scores.append(np.average(fscore)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm,tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "years = [\"2014-15\",\"2014-16\",\"2014-17\"]\n",
    "plt.scatter(years, f_scores)\n",
    "plt.title(\"Training accuarcy of classifier(SVM) on tweets for different time range\",fontsize = 20)\n",
    "plt.xlabel(\"Range of year span \",fontsize = 16)\n",
    "plt.ylabel(\"f1-score\",fontsize = 16)\n",
    "plt.xticks(rotation=30, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  bucket 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2018\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "print(\"tweets\",len(tweets))\n",
    "print(\"users\",len(tweets.userID.unique()))\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "_,_,fscore,_ = precision_recall_fscore_support(Y_test,Y_pred)\n",
    "# print(np.average(fscore))\n",
    "f_scores.append(np.average(fscore)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm,tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "years = [\"2014-15\",\"2014-16\",\"2014-17\",\"2014-18\"]\n",
    "plt.plot(years, f_scores)\n",
    "plt.title(\"Training accuarcy of classifier(SVM) on tweets for different time range\",fontsize = 20)\n",
    "plt.xlabel(\"Range of year span \",fontsize = 16)\n",
    "plt.ylabel(\"f1-score\",fontsize = 16)\n",
    "plt.xticks(rotation=30, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change of accuracy of the svm model using only the specific year data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## prediction\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2015]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2016\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2017\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2018\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2019\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looking at the cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def get_depth(G):\n",
    "    level = nx.get_node_attributes(G,'level')\n",
    "    if not level:\n",
    "        level = nx.get_node_attributes(G,'depth')\n",
    "    depth = max(level.items(), key=operator.itemgetter(1))[1]\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = os.path.join(top_dir,\"models\",\"graphs2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [file for file in os.listdir(os.path.join(os.getcwd(),graph_path)) if file.endswith(\".gpickle\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_nodes(G,main_node,poly_users):\n",
    "    poly_nodes = []\n",
    "    mono_nodes = []\n",
    "    for node in G:\n",
    "        if node != main_node:\n",
    "            if node in set(poly_users):\n",
    "                poly_nodes.append(node)\n",
    "            else:\n",
    "                mono_nodes.append(node)\n",
    "    return (poly_nodes,mono_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the influence in the cascades..\n",
    "## influence = no of poly/mono / total no of nodes\n",
    "influence_poly = []\n",
    "influence_mono = []\n",
    "for file in filenames:\n",
    "    G = nx.read_gpickle(os.path.join(graph_path,file))\n",
    "    depth = nx.get_node_attributes(G,'level')\n",
    "    main_node = [node for node,level in depth.items() if level ==0]\n",
    "    poly_nodes,mono_nodes =  get_type_nodes(G,main_node,poly_users)\n",
    "#     if len(G.nodes) > 30:\n",
    "    if main_node[0] in set(poly_users):\n",
    "        influence_poly.append(len(poly_nodes)/(len(poly_nodes)+len(mono_nodes)))\n",
    "    else:\n",
    "        influence_mono.append(len(mono_nodes)/(len(poly_nodes)+len(mono_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(influence_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(influence_mono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(influence_mono) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(influence_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poly_users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
