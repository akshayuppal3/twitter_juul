{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew_william2345/anaconda3/envs/multimodal-env/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy \n",
    "import os\n",
    "import git\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "# import gensim\n",
    "# from gensim import corpora\n",
    "import os\n",
    "import networkx as nx\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Average\n",
    "from keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the labelled files and the poly_user\n",
    "def get_git_root(path):\n",
    "\tgit_repo = git.Repo(path, search_parent_directories=True)\n",
    "\tgit_root = git_repo.git.rev_parse(\"--show-toplevel\")\n",
    "\treturn git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/drew_william2345/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/drew_william2345/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# setup env\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize the text..\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tknzr = TweetTokenizer()\n",
    "def get_tokens(sentence):\n",
    "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
    "    tokens = tweet_tknzr.tokenize(sentence)\n",
    "    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1)]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'(https?://\\S+)', \"\", text) ## remove url\n",
    "    text = re.sub(r'(\\@\\w+)', \"author\",text)   ## remove @ mentions with author\n",
    "    text = re.sub(r'(@)', \"\",text)             ## remove @ symbols\n",
    "    text = re.sub(r'(author)',\"\",text)         ## remove author\n",
    "    text = re.sub(r'(#)', \"\",text)             ## removing the hashtags signal \n",
    "    text = re.sub(r'(RT )', \"\",text)         ## remove the retweet info as they dont convey any information\n",
    "    text.rstrip \n",
    "    text.lstrip\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the word2vec dict from the dictionary\n",
    "def get_word2vec(file_path):\n",
    "    file = open(file_path, \"r\")\n",
    "    if (file):\n",
    "        word2vec = dict()\n",
    "#         split = file.read().splitlines()\n",
    "        for line in file:\n",
    "            split_line = line.split(' ')\n",
    "            key = split_line[0] # the first word is the key\n",
    "            value = np.array([float(val) for val in split_line[1:]])\n",
    "            word2vec[key] = value\n",
    "        return (word2vec)\n",
    "    else:\n",
    "        print(\"invalid fiel path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the mean of the embeddings\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "\tdef __init__(self, word2vec):\n",
    "\t\tself.word2vec = word2vec\n",
    "\t\t# if a text is empty we should return a vector of zeros\n",
    "\t\t# with the same dimensionality as all the other vectors\n",
    "\t\tself.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\treturn np.array([\n",
    "\t\t\tnp.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "\t\t\t\t\tor [np.zeros(self.dim)], axis=0)\n",
    "\t\t\tfor words in tqdm(X)\n",
    "\t\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the embedding matrix weights:\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def get_embedding_matrix(keras_tkzr,word2vec):\n",
    "    vocab_size = len(keras_tkzr.word_index) + 1\n",
    "    embedding_matrix = zeros((vocab_size, 100))\n",
    "    for word, i in keras_tkzr.word_index.items():\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return (embedding_matrix,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pass to the bi-lstm model\n",
    "def create_model(max_len,vocab_size,embedding_matrix):\n",
    "    input = Input(shape=(max_len,))\n",
    "    model = Embedding(vocab_size,100,weights=[embedding_matrix],input_length=max_len)(input)\n",
    "    model =  Bidirectional (LSTM (100,return_sequences=True,dropout=0.50),merge_mode='concat')(model)\n",
    "    model = TimeDistributed(Dense(100,activation='relu'))(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(100,activation='relu')(model)\n",
    "    output = Dense(3,activation='softmax')(model)\n",
    "    model = Model(input,output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "def load_data(df,max_len):\n",
    "    keras_tkzr = Tokenizer()\n",
    "    le = LabelEncoder()\n",
    "    keras_tkzr.fit_on_texts(df[\"tweetText\"])\n",
    "    ## getting input X\n",
    "    token_list = list(df['tweetText'].apply(get_tokens))\n",
    "    encoded_docs = keras_tkzr.texts_to_sequences(token_list)\n",
    "    max_len = max_len\n",
    "    X = pad_sequences(encoded_docs, maxlen=max_len, padding='post')\n",
    "    ## getting output Y\n",
    "    Y = (list(df['label']))\n",
    "    le.fit(Y)\n",
    "    le.classes_\n",
    "    y = le.transform(Y)\n",
    "    return (X,y,keras_tkzr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the trained model\n",
    "def train_evaluate_model(model,X_train,Y_train,X_test,Y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_pred = np.array([np.argmax(pred) for pred in Y_pred])\n",
    "    # scores = cross_val_score(model, X_test, Y_test, cv=5)\n",
    "    print('  Classification Report:\\n',classification_report(Y_test,y_pred),'\\n')\n",
    "    precision,recall,fscore,_ = precision_recall_fscore_support(Y_test,y_pred)\n",
    "    return (model,precision,recall,fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model,X_train,Y_train,epoch):\n",
    "    hist = model.fit(X_train,Y_train,validation_split=0.25, nb_epoch = epoch, verbose = 2)\n",
    "    return model,hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dir = os.path.join(get_git_root(os.getcwd()))\n",
    "input_dir = os.path.join(get_git_root(os.getcwd()),\"input\")\n",
    "embeddings_dir  = os.path.join(get_git_root(os.getcwd()),\"input\",\"embeddings\")\n",
    "annotatted_dir = os.path.join(input_dir,\"annotated_data\")\n",
    "classifier_dir = os.path.join(get_git_root(os.getcwd()),\"models\",\"classifier\")\n",
    "model_dir = os.path.join(get_git_root(os.getcwd()),\"models\")\n",
    "poly_dir = os.path.join(model_dir,\"poly_users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = os.path.join(embeddings_dir,\"glove.twitter.27B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_following_final = pd.read_csv(os.path.join(input_dir,\"following_final.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(os.path.join(input_dir,\"dataset.csv\"),lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11578887\n",
      "551261\n"
     ]
    }
   ],
   "source": [
    "print(len(df_dataset))   # 11,578,887\n",
    "print(len(df_dataset.userID.unique())) # 551,261"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetCreatedAt</th>\n",
       "      <th>tweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112655608</td>\n",
       "      <td>550592152947150848</td>\n",
       "      <td>2015-01-01 09:59:33</td>\n",
       "      <td>ON AIR : Rasmus Juul &amp;amp; Kasual - Got No Rhythm (Mark Lower Remix) - (http://t.co/w9HX9OSoND)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>405051688</td>\n",
       "      <td>550774424358887424</td>\n",
       "      <td>2015-01-01 22:03:50</td>\n",
       "      <td>Listen to Zella Day - Seven Nation Army ϟ Kasúal &amp;amp; Rasmus Juul Edit by Kasúal #np on #SoundCloud ❤️ https://t.co/zIY3KNP1Cb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1442149842</td>\n",
       "      <td>550796793936896000</td>\n",
       "      <td>2015-01-01 23:32:43</td>\n",
       "      <td>@halahoran so proud of you juul i love you lots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1615154773</td>\n",
       "      <td>551081300049006592</td>\n",
       "      <td>2015-01-02 18:23:15</td>\n",
       "      <td>@vitaminiam that's so good, i have that with juul hahaha (she's from nuenen)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13163002</td>\n",
       "      <td>551127239321206785</td>\n",
       "      <td>2015-01-02 21:25:47</td>\n",
       "      <td>\"We should care enough about winning to put in sufficient effort, but we should see learning rather than winning the ultimate goal\" J. Juul</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userID             tweetId       tweetCreatedAt  \\\n",
       "0  112655608   550592152947150848  2015-01-01 09:59:33   \n",
       "1  405051688   550774424358887424  2015-01-01 22:03:50   \n",
       "2  1442149842  550796793936896000  2015-01-01 23:32:43   \n",
       "3  1615154773  551081300049006592  2015-01-02 18:23:15   \n",
       "4  13163002    551127239321206785  2015-01-02 21:25:47   \n",
       "\n",
       "                                                                                                                                     tweetText  \n",
       "0  ON AIR : Rasmus Juul &amp; Kasual - Got No Rhythm (Mark Lower Remix) - (http://t.co/w9HX9OSoND)                                              \n",
       "1  Listen to Zella Day - Seven Nation Army ϟ Kasúal &amp; Rasmus Juul Edit by Kasúal #np on #SoundCloud ❤️ https://t.co/zIY3KNP1Cb              \n",
       "2  @halahoran so proud of you juul i love you lots                                                                                              \n",
       "3  @vitaminiam that's so good, i have that with juul hahaha (she's from nuenen)                                                                 \n",
       "4  \"We should care enough about winning to put in sufficient effort, but we should see learning rather than winning the ultimate goal\" J. Juul  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting promoter, regular, news from data using bilstm model -- finally use svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the svm vectorizer trained on annottaion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_annotation)\n",
    "# ## 0 -news\n",
    "# ## 1 - promo\n",
    "# ## 2 -regular users\n",
    "# print(df_annotation.label.value_counts())\n",
    "# df_annotation = df_annotation.sample(frac=1)\n",
    "# df_annotation = df_annotation.reset_index()\n",
    "# ## take a balnced sample of 200 from each category\n",
    "# df_news = (df_annotation[df_annotation.label == 0])[:200] \n",
    "# df_reg = (df_annotation[df_annotation.label == 2])[:200] \n",
    "# df_promo = (df_annotation[df_annotation.label == 1])[:200] \n",
    "# df_annotation = pd.concat([df_news,df_promo,df_reg])\n",
    "# ## random sample annotation file\n",
    "# df_annotation = df_annotation.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# ## tokenize the sentences\n",
    "# df_annotation['tweetText'] = df_annotation['tweetText'].apply(clean_text)\n",
    "# df_annotation.loc[df_annotation.label.isin([1,2])].head(200)\n",
    "\n",
    "# ## looking at the tokenized data\n",
    "# df_test = pd.DataFrame()\n",
    "# df_test[\"tweetText\"] = list(df_annotation['tweetText'].apply(get_tokens))\n",
    "# df_test['label'] = list(df_annotation[\"label\"])\n",
    "# df_test\n",
    "\n",
    "## getting mean , quantile length of text\n",
    "# df_len = list()\n",
    "# for tokens in token_list:\n",
    "#     df_len.append(len(tokens))\n",
    "# df_len = pd.DataFrame(df_len)\n",
    "# df_len.boxplot()\n",
    "# df_len.describe()\n",
    "# max_len = int(df_len.quantile(0.95))  ## take the max_len = 90 percentile\n",
    "# max_len = 27\n",
    "\n",
    "## we use svm for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11578887"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model and tokenizer\n",
    "svm_tf = pickle.load(open(os.path.join(classifier_dir,\"annotation_classifier\",\"svm_tf.pkl\"),\"rb\"))\n",
    "tfidf_vect = pickle.load(open(os.path.join(classifier_dir,\"annotation_classifier\",\"tf_idf_vect.pkl\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_dataset[\"tweetText\"]\n",
    "X = tfidf_vect.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = svm_tf.predict(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11578887, 3789)\n",
      "(11578887,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset[\"label\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4901646419038376\n",
      "5675561\n"
     ]
    }
   ],
   "source": [
    "## regualr tweets\n",
    "use= len(df_dataset.loc[df_dataset.label == 2])\n",
    "print(use/len(df_dataset))\n",
    "print(use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38150722085810146\n"
     ]
    }
   ],
   "source": [
    "## news tweets\n",
    "news = len(df_dataset.loc[df_dataset.label == 0])\n",
    "print(news/ len(df_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12832813723806097\n"
     ]
    }
   ],
   "source": [
    "## promotional tweets\n",
    "prom = len(df_dataset.loc[df_dataset.label == 1])\n",
    "print(prom/ len(df_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting regular, promoters and news users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5675561.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.get_dummies(df_dataset,columns=[\"label\"],prefix=['cat'])\n",
    "users_type = temp.groupby(by=\"userID\").agg({'cat_0': 'sum','cat_1':'sum','cat_2':'sum'})\n",
    "users_type = users_type.reset_index()\n",
    "users_type[\"label\"] = users_type[['cat_0','cat_1','cat_2']].idxmax(axis=1)\n",
    "## sanity check\n",
    "# regular tweets 5675561\n",
    "(np.sum(list(users_type[\"cat_2\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot of user, promoter and news.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total users 551261\n",
      "regular_users 432316\n",
      "promoters users 31578\n",
      "news users, 87367\n",
      "551261\n",
      "551261\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## no of users in each category\n",
    "print(\"total users\",len(users_type))\n",
    "print(\"regular_users\",len(users_type.loc[users_type.label == \"cat_2\"]))\n",
    "print(\"promoters users\",len(users_type.loc[users_type.label == \"cat_1\"]))\n",
    "print(\"news users,\",len(users_type.loc[users_type.label == \"cat_0\"]))\n",
    "users_ = {\"reg\":len(users_type.loc[users_type.label == \"cat_2\"]),\n",
    " \"prom\":len(users_type.loc[users_type.label == \"cat_1\"]),\n",
    " \"news\":len(users_type.loc[users_type.label == \"cat_0\"])}\n",
    "\n",
    "## sanity check\n",
    "print(np.sum(list(users_.values())))\n",
    "print(len(users_type.userID.unique()))\n",
    "\n",
    "## plotting each user by category\n",
    "fig, ax = plt.subplots(figsize=(10,5)) \n",
    "ax.set_title(\"distrbution of users\")\n",
    "ax.bar(users_.keys(),users_.values())\n",
    "for idx,(key,value) in enumerate(users_.items()):\n",
    "    ax.text( idx - 0.2 ,value + 0.5, value, color='blue',fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the normal users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting all the tweets from normal users(who have max number of user expressed tweets)\n",
    "regular_users = list(users_type[\"userID\"].loc[users_type.label == \"cat_2\"])\n",
    "normal_tweets = (df_dataset.loc[df_dataset.userID.isin(regular_users)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_type.to_csv(os.path.join(input_dir,\"labelled_data\",\"users_type.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.to_csv(os.path.join(input_dir,\"labelled_data\",\"dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432316"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(regular_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6512011"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting poly and mono users.. (check with most common words as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokens_list):\n",
    "    vocab = Counter()\n",
    "    for tokens in tokens_list:\n",
    "        vocab.update(tokens)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return clean text\n",
    "def prepare_text_LDA(sentence):\n",
    "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
    "    tweet_tknzr = TweetTokenizer()\n",
    "    tokens = tweet_tknzr.tokenize(sentence)\n",
    "    tokens = [token.lower() for token in tokens if (token not in stopwords and token.isalpha())] # stopwords removal\n",
    "    tokens = [get_lemma(token) for token in tokens]  # lemmatization\n",
    "    return (\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join al the of the tweets for each user\n",
    "def get_tweets_user(df):\n",
    "    df_tweets_comb = list()\n",
    "    users = df.userID.unique()\n",
    "    for user in users:\n",
    "        tweets =(df.loc[df.userID == user])\n",
    "        tweets_f = (\" . \".join(tweets[\"tweetText\"]))\n",
    "        df_tweets_comb.append((user,tweets_f))\n",
    "    df_user = pd.DataFrame(df_tweets_comb, columns=[\"userID\",\"tweetText\"])\n",
    "    return (df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting the pattern for weeds\n",
      "extracting the pattern for juul\n"
     ]
    }
   ],
   "source": [
    "## getting the weed and juul pattern\n",
    "print(\"extracting the pattern for weeds\")\n",
    "weed_words = pickle.load(open(os.path.join(model_dir, \"weed_words.pkl\"), \"rb\"))\n",
    "# weed_words.remove('grass')\n",
    "# weed_words.remove('pot')\n",
    "# weed_words.remove('smoke')\n",
    "# weed_words.remove('bud')\n",
    "# weed_words.remove('joint')\n",
    "#  weed_words.remove('CBD oil')\n",
    "# weed_words = [(\" \" + word + \" \") for word in weed_words]\n",
    "pattern_weed = \"|\".join(weed_words)\n",
    "print(\"extracting the pattern for juul\")\n",
    "pattern_juul = 'juul'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weed|ganja|marijuana|cannabis|mary jane|hemp|marihuana|hash|reefer|hashish|herb|bhang|green goddess|locoweed|maryjane|spliff|wacky baccy|sinsemilla|doobie|acapulco gold|CBD|THC'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_weed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2826b6baeb4ed095e826b2b74a8361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6512011), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 48%|████▊     | 3147771/6512011 [00:42<00:24, 137852.47it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "## preprocesing the tweets\n",
    "tqdm.pandas()\n",
    "normal_tweets[\"tweetText\"] = normal_tweets[\"tweetText\"].progress_apply(clean_text)\n",
    "# normal_tweets[\"tweetText\"] = (normal_tweets[\"tweetText\"].progress_apply(prepare_text_LDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weed tweets =  5500891\n",
      "no of poly users =  431942\n"
     ]
    }
   ],
   "source": [
    "## load the weed tweets\n",
    "weed_tweets = normal_tweets[normal_tweets['tweetText'].str.contains(pattern_weed, case=False)]\n",
    "print(\"weed tweets = \",len(weed_tweets))\n",
    "poly_users = list(weed_tweets.userID.unique())\n",
    "print(\"no of poly users = \", len(poly_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total users =  432316\n",
      "no of poly users =  431942\n",
      "no of mono users =  374\n",
      "% of poly users is  0.9991348920696898\n",
      "% of mono users is  0.000865107930310236\n"
     ]
    }
   ],
   "source": [
    "normal_users = list(normal_tweets.userID.unique())\n",
    "poly_length = len(poly_users)\n",
    "total_users_length = len(normal_users)\n",
    "mono_length = total_users_length - poly_length\n",
    "print(\"total users = \", total_users_length)\n",
    "print(\"no of poly users = \", poly_length)\n",
    "print(\"no of mono users = \", mono_length)\n",
    "print(\"% of poly users is \", poly_length / total_users_length)\n",
    "print(\"% of mono users is \", mono_length / total_users_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dump the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def dump_obj(obj,pathname):\n",
    "    with open(pathname,\"wb\") as f:\n",
    "        pickle.dump(obj,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the poly_users,regualar_users\n",
    "poly_dir = os.path.join(model_dir,\"poly_users\")\n",
    "pathname = os.path.join(poly_dir,\"poly_users.pkl\")\n",
    "dump_obj(poly_users,pathname)\n",
    "pathname = os.path.join(poly_dir,\"normal_users.pkl\")\n",
    "dump_obj(regular_users,pathname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the poly - mono users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_users = pickle.load(open(os.path.join(poly_dir,\"poly_users.pkl\"),\"rb\"))\n",
    "normal_users = pickle.load(open(os.path.join(poly_dir,\"normal_users.pkl\"),\"rb\"))\n",
    "df_dataset = pd.read_csv(os.path.join(input_dir,\"labelled_data\",\"tweets_predicted.csv\"),lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_tweets = df_dataset.loc[df_dataset.userID.isin(normal_users)]\n",
    "poly_tweets = df_dataset.loc[df_dataset.userID.isin(poly_users)]\n",
    "mono_users = list(set(normal_users) - set(poly_users))\n",
    "mono_tweets = df_dataset.loc[df_dataset.userID.isin(mono_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tweets by poly users:  6510725\n",
      "poly users (based with retweets): 431942\n"
     ]
    }
   ],
   "source": [
    "print(\"all tweets by poly users: \", len(poly_tweets))\n",
    "print(\"poly users (based with retweets):\", len(poly_tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tweets by mono users:  1286\n",
      "mono users (based with retweets): 374\n"
     ]
    }
   ],
   "source": [
    "print(\"all tweets by mono users: \", len(mono_tweets))\n",
    "print(\"mono users (based with retweets):\", len(mono_tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning the text\n",
    "poly_tweets[\"tweetText\"] = poly_tweets[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets[\"tweetText\"] = mono_tweets[\"tweetText\"].apply(clean_text)\n",
    "## tokenizing and lemmatizing\n",
    "poly_tweets[\"tweetText\"] = (poly_tweets[\"tweetText\"].apply(prepare_text_LDA))\n",
    "mono_tweets[\"tweetText\"] = mono_tweets[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting most coomon words for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_tweets_user = get_tweets_user(poly_tweets)\n",
    "mono_tweets_user = get_tweets_user(mono_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_p = list(poly_tweets_user[\"tweetText\"].apply(get_tokens))\n",
    "tokens_j = list(mono_tweets_user[\"tweetText\"].apply(get_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(tokens_p)\n",
    "vocab.most_common()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## most common words for juul user tweets\n",
    "vocab1 = build_vocab(tokens_j)\n",
    "vocab1.most_common()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = os.path.join(model_dir,\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokens_p)  # tokens_j\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens_p] # tokens_j\n",
    "pickle.dump(corpus, open(os.path.join(temp_dir,'corpus_m.pkl'), 'wb'))\n",
    "dictionary.save(os.path.join(temp_dir,'dictionary_m.gensim'))\n",
    "## LDA\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=8)\n",
    "ldamodel.save(os.path.join(temp_dir,'model_m.gensim'))\n",
    "topics = ldamodel.print_topics(num_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## poly\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(temp_dir,'dictionary_m.gensim'))\n",
    "corpus = pickle.load(open(os.path.join(temp_dir,'corpus_m.pkl'), 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load(os.path.join(temp_dir,'model_m.gensim'))\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### mono\n",
    "dictionary = corpora.Dictionary(tokens_j)  # tokens_j\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens_j] # tokens_j\n",
    "pickle.dump(corpus, open(os.path.join(temp_dir,'corpus_m.pkl'), 'wb'))\n",
    "dictionary.save(os.path.join(temp_dir,'dictionary_m.gensim'))\n",
    "## LDA\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=8)\n",
    "ldamodel.save(os.path.join(temp_dir,'model_m.gensim'))\n",
    "topics = ldamodel.print_topics(num_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mono\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(temp_dir,'dictionary_m.gensim'))\n",
    "corpus = pickle.load(open(os.path.join(temp_dir,'corpus_m.pkl'), 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load(os.path.join(temp_dir,'model_m.gensim'))\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training_classifier to predict poly vs mono user -- no longer valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get further categorization of poly_subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @return the posotion of juul timeline in the list of weeds\n",
    "def get_postion(times_j, list_w):\n",
    "    if list_w is not None:\n",
    "        pos = -1\n",
    "        for idx,ele in enumerate(list_w):\n",
    "            if (times_j) > ele:\n",
    "                continue\n",
    "            else:\n",
    "                pos = idx\n",
    "                break\n",
    "        if pos == -1:\n",
    "            pos = len(list_w)\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551261"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dataset.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weed first and juul first for each of the poly users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** starting with the poly sub type users\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5396b007feb4c7596538e4c53cdd62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=431942), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-f1db189131bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweed_tweets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             list_w = pd.to_datetime(list(weed_tweets[\n\u001b[0;32m---> 21\u001b[0;31m                                               'tweetCreatedAt']))\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlist_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_and_box_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"*** starting with the poly sub type users\")\n",
    "juul_before = list()\n",
    "juul_after = list()\n",
    "# poly_middle = list()\n",
    "_und = list()\n",
    "total_users = list(df_dataset.userID.unique())\n",
    "for user in tqdm(poly_users):\n",
    "    user_tweets = df_dataset.loc[df_dataset.userID == user]\n",
    "    user_tweets = user_tweets.sort_values(by='tweetCreatedAt', ascending=True)  # sort by tweet created at\n",
    "    times_j = None\n",
    "    list_w = None\n",
    "    j_anchor = list(user_tweets['tweetCreatedAt'].loc[user_tweets['tweetText'].str.contains(pattern_juul, case=False)]) ## get the first occurance\n",
    "    if j_anchor:\n",
    "        time_j = pd.to_datetime(j_anchor[0])\n",
    "    else:\n",
    "        time_j = None\n",
    "    weed_tweets = user_tweets[user_tweets['tweetText'].str.contains(pattern_weed, case=False)]\n",
    "    if (len(weed_tweets) > 0):\n",
    "        if (len(weed_tweets) > 0):\n",
    "            list_w = pd.to_datetime(list(weed_tweets[\n",
    "                                              'tweetCreatedAt']))\n",
    "    else:\n",
    "        list_w = None\n",
    "    if (time_j is not None and list_w is not None):\n",
    "        pos = get_postion(time_j,list_w)              ## @TODO change..create function\n",
    "        if (pos == 0):\n",
    "            juul_before.append(user)\n",
    "        else:\n",
    "            juul_after.append(user)\n",
    "    else:\n",
    "        _und.append(user)                         ## if we can't determine the juul tweets in the user..\n",
    "print(\"Poly type users calculated\")\n",
    "print(\"total poly users =\", len(poly_users))\n",
    "print(\"****************\\n\")\n",
    "print(\"% of juul before users = \", len(juul_before) / len(poly_users))\n",
    "print(\"len of juul before = \",len(juul_before))\n",
    "print(\"\\n\")\n",
    "print(\"% of juul after users = \", len(juul_after) / len(poly_users))\n",
    "print(\"len of juul after = \",len(juul_after))\n",
    "print(\"\\n\")\n",
    "print(\"len of undefined users = \", len(_und) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poly type users calculated\n",
    "total users = 640\n",
    "\n",
    "****************\n",
    "\n",
    "% of juul before users =  0.190625\n",
    "len of juul before =  122\n",
    "\n",
    "\n",
    "% of juul after users =  0.809375\n",
    "len of juul after =  518\n",
    "\n",
    "\n",
    "% of undefined users =  0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting the change of poly_before to poly again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the poly_before user juul before and first weed tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(juul_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juul_before_tweets = df_final.loc[df_final.userID.isin(juul_before)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(juul_before_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juul_tweets = juul_before_tweets.loc[juul_before_tweets[\"tweetText\"].str.contains(pattern_juul,case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all the juul before users contain juul tweets\n",
    "len(juul_tweets.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juul_tweets[\"tweetCreatedAt\"] = pd.to_datetime(juul_tweets[\"tweetCreatedAt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed = []\n",
    "for user in juul_before:\n",
    "    user_tweets = juul_tweets.loc[juul_tweets.userID == user]\n",
    "    user_tweets = user_tweets.sort_values(by=[\"tweetCreatedAt\"])\n",
    "    if (list(user_tweets[\"tweetCreatedAt\"])):\n",
    "        anchor_juul = list(user_tweets[\"tweetCreatedAt\"])[0]\n",
    "        poly_first_juul_weed.append([user,anchor_juul])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed = pd.DataFrame(poly_first_juul_weed,columns=[\"userID\",\"juul_first\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = juul_before_tweets.loc[juul_before_tweets[\"tweetText\"].str.contains(pattern_weed,case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_first = []\n",
    "for user in juul_before:\n",
    "    user_tweets = weed_tweets.loc[weed_tweets.userID == user]\n",
    "    user_tweets = user_tweets.sort_values(by=[\"tweetCreatedAt\"])\n",
    "    if (list(user_tweets[\"tweetCreatedAt\"])):\n",
    "        anchor_weed = list(user_tweets[\"tweetCreatedAt\"])[0]\n",
    "        anchor_weed = pd.to_datetime(anchor_weed)\n",
    "        weed_first.append(anchor_weed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed[\"weed_first\"] = weed_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed = poly_first_juul_weed.sort_values(by=[\"juul_first\"],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "users_list = list(poly_first_juul_weed.userID.unique())\n",
    "plt.scatter(list(poly_first_juul_weed[\"juul_first\"]),[str(user) for user in users_list],color=\"blue\")\n",
    "plt.scatter(list(poly_first_juul_weed[\"weed_first\"]), [str(user) for user in users_list],color=\"green\")\n",
    "plt.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    left=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelleft=False) # labels along the bottom edge are of\n",
    "plt.ylabel(\"users\", fontsize = 15)\n",
    "plt.xlabel(\"timeline \", fontsize=15)\n",
    "plt.title(\"timeline of change of users from mono to poly\" )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_first_juul_weed[\"interval\"] = poly_first_juul_weed[\"weed_first\"].dt.date - poly_first_juul_weed[\"juul_first\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [ele.days() for ele in list(poly_first_juul_weed[\"interval\"])]\n",
    "days.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "counts = {}\n",
    "delta = 5000\n",
    "counts[(pos+delta)] = 0\n",
    "for ele in days:\n",
    "    if  ele < pos + delta:\n",
    "        counts[(pos+ delta)] += 1\n",
    "    else:\n",
    "        pos += delta\n",
    "        counts[(pos + delta)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [ele[1] for ele in sorted(counts.items(), key=lambda x: x[0])]\n",
    "keys = [ele[0] for ele in sorted(counts.items(), key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.bar(keys,values,width = 2)\n",
    "plt.title(\"Delay between juul first and substance tweet\")\n",
    "plt.xlabel(\"Delay (days)\")\n",
    "plt.ylabel(\"Delay\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gettng the network properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_following_final = pd.read_csv(os.path.join(input_dir,\"following_final.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(normal_users))\n",
    "print(len(poly_users))\n",
    "print(len(mono_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## selecting a subset of the following that contain the total users\n",
    "df_poly_users = df_following_final[df_following_final.userID.isin(poly_users)]\n",
    "df_mono_users = df_following_final[df_following_final.userID.isin(mono_users)]\n",
    "df_all_users = df_following_final[df_following_final.userID.isin(total_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(df):\n",
    "    G = nx.DiGraph()\n",
    "    users = list(df.userID.unique())\n",
    "    for user in tqdm(users):\n",
    "        following_A = set(ast.literal_eval((df.loc[df.userID == user].head(1)[\"following\"].values)[0]))\n",
    "        user_set = set([node for node in users if node != user])\n",
    "        users_list = user_set.intersection(following_A)\n",
    "        for user_following in list(users_list):\n",
    "            G.add_edge(user,user_following)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_poly = get_graph(df_poly_users)\n",
    "G_mono = get_graph(df_mono_users)\n",
    "G_all = get_graph(df_all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color map \n",
    "def get_color_map(G,poly_users):\n",
    "    color_map = list()\n",
    "    for node in G.nodes():\n",
    "        if int(node) in set(poly_users):\n",
    "            color_map.append('green')\n",
    "        else:\n",
    "            color_map.append('blue')\n",
    "    return color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mono viz --(filetersd)\n",
    "color_map = get_color_map(G_mono,poly_users)\n",
    "plt.figure(figsize=(30,30))\n",
    "nx.draw(G_mono,node_color=color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = get_color_map(G_poly,poly_users)\n",
    "plt.figure(figsize=(30,30))\n",
    "nx.draw(G_poly,node_color=color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "color_map = get_color_map(G_all,poly_users)\n",
    "pos = nx.spring_layout(G_all)\n",
    "nx.draw_networkx(G_all, pos=pos, node_color = color_map,with_label = False,arrows=True,labels=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G_all))\n",
    "print(\"average clustering poly\",nx.average_clustering(G_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G_mono))\n",
    "print(\"average clustering poly\",nx.average_clustering(G_mono))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G_poly))\n",
    "print(\"average clustering mono\",nx.average_clustering(G_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the change of mono as(poly_before) to poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date_format = \"%Y-%m-%d\"\n",
    "time_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "my_date = datetime.datetime.strptime(\"2015-01-03\",date_format)\n",
    "final_date = datetime.datetime.combine(my_date,datetime.time(10,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_date.strftime(time_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bucket the users in bucket :\n",
    "#1 2014 - [2015-2018] \n",
    "#2 2014- 2015 - [16 - 18]\n",
    "#3 2014 - 2016 - [17-18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_hexagon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all = df_tweets.loc[df_tweets.userID.isin(normal_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_all.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first bucket \n",
    "tweets_all[\"tweetCreatedAt\"] = pd.to_datetime(tweets_all.tweetCreatedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2014 = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(tweets_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2014.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining all of the tweets of the user\n",
    "tweets_2014 = get_tweets_user(tweets_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for idx,row in tweets_2014.iterrows():\n",
    "    if row['userID'] in poly_users:\n",
    "        label.append(\"poly\")\n",
    "    else:\n",
    "        label.append(\"mono\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2014[\"label\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "tweets_2014[\"tweetText\"] = tweets_2014[\"tweetText\"].apply(clean_text)\n",
    "tweets_2014[\"tweetText\"] = tweets_2014[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load the model and tokenizer\n",
    "lstm_model = pickle.load(open(os.path.join(poly_dir,\"pomo_classifier_lstm.pkl\"),\"rb\"))\n",
    "keras_tkzr = pickle.load(open(os.path.join(poly_dir,\"keras_tkzr.pkl\"),\"rb\"))\n",
    "le = pickle.load(open(os.path.join(poly_dir,\"label_encoder.pkl\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(tweets_2014[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in lstm_model.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Y_true == Y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taking only mono users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the keywords to gather all of the mono users out of the poly ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"extracting the pattern for weeds\")\n",
    "weed_words = pickle.load(open(os.path.join(model_dir, \"weed_words.pkl\"), \"rb\"))\n",
    "pattern_weed = \"|\".join(weed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_weed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = tweets_2014[tweets_2014[\"tweetText\"].str.contains(pattern_weed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = tweets_2014.userID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_user_a = weed_tweets.userID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poly_user_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_users_a = list(set(total_users)  - set(set(poly_user_a))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mono_users_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_tweets_2014 = tweets_2014.loc[tweets_2014.userID.isin(mono_users_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mono_tweets_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for idx,row in mono_tweets_2014.iterrows():\n",
    "    if row['userID'] in poly_users:\n",
    "        label.append(\"poly\")\n",
    "    else:\n",
    "        label.append(\"mono\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model and tokenizer\n",
    "pomo_lstm = pickle.load(open(os.path.join(poly_dir,\"pomo_classifier_lstm.pkl\"),\"rb\"))\n",
    "keras_tkzr = pickle.load(open(os.path.join(poly_dir,\"keras_tkzr.pkl\"),\"rb\"))\n",
    "le = pickle.load(open(os.path.join(poly_dir,\"label_encoder.pkl\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "mono_tweets_2014[\"tweetText\"] = mono_tweets_2014[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets_2014[\"tweetText\"] = mono_tweets_2014[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ele for ele in Y_true if ele == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(mono_tweets_2014[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in pomo_lstm.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels: \n",
    "mono 0\n",
    "poly 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2015 = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = (tweets_2015.userID.unique())\n",
    "len(total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining all of the tweets of the user\n",
    "tweets_2015 = get_tweets_user(tweets_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = tweets_2015[tweets_2015[\"tweetText\"].str.contains(pattern_weed)]\n",
    "len(weed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_user_a = (weed_tweets.userID.unique())\n",
    "len(poly_user_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_users_a = list(set(total_users)  - set(set(poly_user_a))) \n",
    "len(mono_users_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_tweets_2015 = tweets_2015[tweets_2015.userID.isin(mono_users_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df,poly_users):\n",
    "    label = []\n",
    "    for idx,row in df.iterrows():\n",
    "        if row['userID'] in poly_users:\n",
    "            label.append(\"poly\")\n",
    "        else:\n",
    "            label.append(\"mono\")\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(mono_tweets_2015,poly_users)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model and tokenizer\n",
    "pomo_lstm = pickle.load(open(os.path.join(poly_dir,\"pomo_classifier_lstm.pkl\"),\"rb\"))\n",
    "keras_tkzr = pickle.load(open(os.path.join(poly_dir,\"keras_tkzr.pkl\"),\"rb\"))\n",
    "le = pickle.load(open(os.path.join(poly_dir,\"label_encoder.pkl\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "mono_tweets_2015[\"tweetText\"] = mono_tweets_2015[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets_2015[\"tweetText\"] = mono_tweets_2015[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mono\",len([ele for ele in Y_true if ele == 0]))\n",
    "print(\"poly\",len([ele for ele in Y_true if ele == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(mono_tweets_2015[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in pomo_lstm.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bucket3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2016 = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = (tweets_2016.userID.unique())\n",
    "len(total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining all of the tweets of the user\n",
    "tweets_2016 = get_tweets_user(tweets_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = tweets_2016[tweets_2016[\"tweetText\"].str.contains(pattern_weed)]\n",
    "len(weed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_user_a = (weed_tweets.userID.unique())\n",
    "len(poly_user_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_users_a = list(set(total_users)  - set(set(poly_user_a))) \n",
    "len(mono_users_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_tweets_2016 = tweets_2016[tweets_2016.userID.isin(mono_users_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(mono_tweets_2016,poly_users)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "mono_tweets_2016[\"tweetText\"] = mono_tweets_2016[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets_2016[\"tweetText\"] = mono_tweets_2016[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mono\",len([ele for ele in Y_true if ele == 0]))\n",
    "print(\"poly\",len([ele for ele in Y_true if ele == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(mono_tweets_2016[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in pomo_lstm.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bucket 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2017 = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = (tweets_2017.userID.unique())\n",
    "len(total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining all of the tweets of the user\n",
    "tweets_2017 = get_tweets_user(tweets_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weed_tweets = tweets_2017[tweets_2017[\"tweetText\"].str.contains(pattern_weed)]\n",
    "len(weed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_user_a = (weed_tweets.userID.unique())\n",
    "len(poly_user_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_users_a = list(set(total_users)  - set(set(poly_user_a))) \n",
    "len(mono_users_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_tweets_2017 = tweets_2017[tweets_2017.userID.isin(mono_users_a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(mono_tweets_2017,poly_users)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(label)\n",
    "print(le.classes_)\n",
    "Y_true = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the tweets\n",
    "mono_tweets_2017[\"tweetText\"] = mono_tweets_2017[\"tweetText\"].apply(clean_text)\n",
    "mono_tweets_2017[\"tweetText\"] = mono_tweets_2017[\"tweetText\"].apply(prepare_text_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mono\",len([ele for ele in Y_true if ele == 0]))\n",
    "print(\"poly\",len([ele for ele in Y_true if ele == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 228\n",
    "encoded_docs = keras_tkzr.texts_to_sequences(mono_tweets_2017[\"tweetText\"])\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(ele) for ele in pomo_lstm.predict(X_new,verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('  Classification Report:\\n',classification_report(Y_true,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "ax.bar(x_label,f1_classifier,width=0.4)\n",
    "plt.title(\"Change of accuarcy of classifier to recongnize mono- > poly on different range of data\",fontsize=14)\n",
    "plt.xlabel(\"date range\",fontsize=12)\n",
    "plt.ylabel(\"f1 score of the classifier\",fontsize=12)\n",
    "for i, v in enumerate(f1_classifier):\n",
    "    plt.text(i - 0.09,v + 0.01, str(v), color='blue', fontweight='bold')\n",
    "#-- need to change with f1 score acuarcy instead of just poly_accuarcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking at the most predictive weights for the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer as keras_Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "keras_tkzr = keras_Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df):\n",
    "    df[\"tweetText\"] = df[\"tweetText\"].apply(clean_text)\n",
    "    df[\"tweetText\"] = df[\"tweetText\"].apply(prepare_text_LDA)\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    # poly_users global\n",
    "    label = []\n",
    "    for idx,row in df.iterrows():\n",
    "        if row['userID'] in poly_users:\n",
    "            label.append(\"poly\")\n",
    "        else:\n",
    "            label.append(\"mono\")\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(label):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(label)\n",
    "    y = le.transform(label)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    label = get_label(df)\n",
    "#     y = encode(label)\n",
    "    tf_idf = TfidfVectorizer(sublinear_tf=True)\n",
    "    tf_idf.fit(df[\"tweetText\"])\n",
    "    feature_names = np.array(tf_idf.get_feature_names())\n",
    "    X = tf_idf.fit_transform(df[\"tweetText\"])\n",
    "    print(X.shape)\n",
    "    return (X,np.array(label),tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_wrapper(X_train,Y_train):\n",
    "    svm = LinearSVC(C=10)\n",
    "    svm.fit(X_train,Y_train,)\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coeff(k,model,feature_names):\n",
    "    coef = (model.coef_.ravel()) \n",
    "    top_positive_coefficients = np.argsort(coef)[-k:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:k]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    colors = ['red' if c < 0 else 'blue' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * k), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 1 + 2 * k), feature_names[top_coefficients], rotation=60, ha='right',fontsize= 20)\n",
    "    plt.show()\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all = df_tweets.loc[df_tweets.userID.isin(normal_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total tweets \",len(tweets_all))\n",
    "print(\"users\", len(tweets_all.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ = clean_tweets(tweets_all)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf_final = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm_final = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(Y_test == Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shape\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocalb size\n",
    "print(len(tf_idf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm_final,tf_idf_final.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "coef = (svm.coef_.ravel()) \n",
    "feature_names = np.array(tf_idf.get_feature_names())\n",
    "top_positive_coefficients = np.argsort(coef)[-k:]\n",
    "list(feature_names[top_positive_coefficients][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_negative_coefficients = np.argsort(coef)[:k]\n",
    "list(feature_names[top_negative_coefficients])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buckets - svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bucket 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all = df_tweets.loc[df_tweets.userID.isin(normal_users)]\n",
    "f_scores = []\n",
    "years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all[\"tweetCreatedAt\"] = pd.to_datetime(tweets_all.tweetCreatedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"tweets\",len(tweets))\n",
    "print(\"users\",len(tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "_,_,fscore,_ = precision_recall_fscore_support(Y_test,Y_pred)\n",
    "# print(np.average(fscore))\n",
    "f_scores.append(np.average(fscore)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm,tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "years = [\"2014-2015\"]\n",
    "plt.scatter(years, f_scores)\n",
    "plt.title(\"Training accuarcy of classifier(SVM) on tweets for different time range\",fontsize = 20)\n",
    "plt.xlabel(\"Range of year span \",fontsize = 16)\n",
    "plt.ylabel(\"f1-score\",fontsize = 16)\n",
    "plt.xticks(rotation=30, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bucket 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"tweets\",len(tweets))\n",
    "print(\"users\",len(tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "_,_,fscore,_ = precision_recall_fscore_support(Y_test,Y_pred)\n",
    "# print(np.average(fscore))\n",
    "f_scores.append(np.average(fscore)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm,tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "years = [\"2014-15\",\"2014-16\"]\n",
    "plt.scatter(years, f_scores)\n",
    "plt.title(\"Training accuarcy of classifier(SVM) on tweets for different time range\",fontsize = 20)\n",
    "plt.xlabel(\"Range of year span \",fontsize = 16)\n",
    "plt.ylabel(\"f1-score\",fontsize = 16)\n",
    "plt.xticks(rotation=30, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"tweets\",len(tweets))\n",
    "print(\"users\",len(tweets.userID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "_,_,fscore,_ = precision_recall_fscore_support(Y_test,Y_pred)\n",
    "# print(np.average(fscore))\n",
    "f_scores.append(np.average(fscore)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm,tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "years = [\"2014-15\",\"2014-16\",\"2014-17\"]\n",
    "plt.scatter(years, f_scores)\n",
    "plt.title(\"Training accuarcy of classifier(SVM) on tweets for different time range\",fontsize = 20)\n",
    "plt.xlabel(\"Range of year span \",fontsize = 16)\n",
    "plt.ylabel(\"f1-score\",fontsize = 16)\n",
    "plt.xticks(rotation=30, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  bucket 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2018\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "print(\"tweets\",len(tweets))\n",
    "print(\"users\",len(tweets.userID.unique()))\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X,y,tf_idf = get_data(tweets_user)\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)\n",
    "svm = svm_wrapper(X_train,Y_train)\n",
    "## svm accuracy on poly mono data ## C=1 kernel= linear\n",
    "Y_pred = svm.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "_,_,fscore,_ = precision_recall_fscore_support(Y_test,Y_pred)\n",
    "# print(np.average(fscore))\n",
    "f_scores.append(np.average(fscore)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coeff = plot_coeff(20,svm,tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "years = [\"2014-15\",\"2014-16\",\"2014-17\",\"2014-18\"]\n",
    "plt.plot(years, f_scores)\n",
    "plt.title(\"Training accuarcy of classifier(SVM) on tweets for different time range\",fontsize = 20)\n",
    "plt.xlabel(\"Range of year span \",fontsize = 16)\n",
    "plt.ylabel(\"f1-score\",fontsize = 16)\n",
    "plt.xticks(rotation=30, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change of accuracy of the svm model using only the specific year data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## prediction\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < 2015]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2016\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2017\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2018\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_c = 2019\n",
    "tweets = tweets_all[tweets_all.tweetCreatedAt.dt.year < year_c]\n",
    "tweets_ = clean_tweets(tweets)\n",
    "tweets_user = get_tweets_user(tweets_)  ## combining all of the tweets of the user\n",
    "X_test = tf_idf_final.transform(tweets_user[\"tweetText\"])\n",
    "Y_true = get_label(tweets_user)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looking at the cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def get_depth(G):\n",
    "    level = nx.get_node_attributes(G,'level')\n",
    "    if not level:\n",
    "        level = nx.get_node_attributes(G,'depth')\n",
    "    depth = max(level.items(), key=operator.itemgetter(1))[1]\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = os.path.join(top_dir,\"models\",\"graphs2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [file for file in os.listdir(os.path.join(os.getcwd(),graph_path)) if file.endswith(\".gpickle\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_nodes(G,main_node,poly_users):\n",
    "    poly_nodes = []\n",
    "    mono_nodes = []\n",
    "    for node in G:\n",
    "        if node != main_node:\n",
    "            if node in set(poly_users):\n",
    "                poly_nodes.append(node)\n",
    "            else:\n",
    "                mono_nodes.append(node)\n",
    "    return (poly_nodes,mono_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the influence in the cascades..\n",
    "## influence = no of poly/mono / total no of nodes\n",
    "influence_poly = []\n",
    "influence_mono = []\n",
    "for file in filenames:\n",
    "    G = nx.read_gpickle(os.path.join(graph_path,file))\n",
    "    depth = nx.get_node_attributes(G,'level')\n",
    "    main_node = [node for node,level in depth.items() if level ==0]\n",
    "    poly_nodes,mono_nodes =  get_type_nodes(G,main_node,poly_users)\n",
    "#     if len(G.nodes) > 30:\n",
    "    if main_node[0] in set(poly_users):\n",
    "        influence_poly.append(len(poly_nodes)/(len(poly_nodes)+len(mono_nodes)))\n",
    "    else:\n",
    "        influence_mono.append(len(mono_nodes)/(len(poly_nodes)+len(mono_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(influence_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(influence_mono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(influence_mono) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(influence_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poly_users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
