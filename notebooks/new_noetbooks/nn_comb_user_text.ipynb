{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import git\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "## tokenize the text..\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer as keras_Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.layers import Embedding\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tknzr = TweetTokenizer()\n",
    "## keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model, Input \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import Embedding \n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Average\n",
    "from keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.models import Model, Input\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the labelled files and the poly_user\n",
    "def get_git_root(path):\n",
    "\tgit_repo = git.Repo(path, search_parent_directories=True)\n",
    "\tgit_root = git_repo.git.rev_parse(\"--show-toplevel\")\n",
    "\treturn git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dir = os.path.join(get_git_root(os.getcwd()))\n",
    "model_dir = os.path.join(get_git_root(os.getcwd()),\"models\")\n",
    "input_dir = os.path.join(get_git_root(os.getcwd()),\"input\")\n",
    "poly_dir = os.path.join(model_dir,\"poly_users\")\n",
    "embeddings_dir = os.path.join(input_dir,\"embeddings\")\n",
    "poly_users = pickle.load(open(os.path.join(poly_dir,\"poly_users.pkl\"),\"rb\"))\n",
    "regular_users = pickle.load(open(os.path.join(poly_dir,\"regular_users.pkl\"),\"rb\"))\n",
    "mono_users = pickle.load(open(os.path.join(poly_dir,\"mono_users.pkl\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input data :juul_data \n",
    "juul_data = pd.read_csv(os.path.join(input_dir,\"juul_data.csv\"),lineterminator=\"\\n\")\n",
    "juul_data[\"tweetCreatedAt\"] = pd.to_datetime(juul_data[\"tweetCreatedAt\"])\n",
    "juul_data = juul_data.loc[juul_data.userID.isin(regular_users)]\n",
    "\n",
    "# ## look at the weed first and juul_first tweet\n",
    "first_data = pd.read_csv(os.path.join(input_dir,\"user_first.csv\"),lineterminator=\"\\n\")\n",
    "first_data = first_data.loc[first_data.userID.isin(regular_users)]  # reg users\n",
    "first_data[\"weed_first\"] = pd.to_datetime(first_data[\"weed_first\"])\n",
    "first_data[\"juul_first\"] = pd.to_datetime(first_data[\"juul_first\"])\n",
    "\n",
    "embedding_file = os.path.join(embeddings_dir,\"glove.twitter.27B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1353987"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## removing the rewteet text column completely\n",
    "ids_ = juul_data.loc[juul_data['retweetText'].notnull()][\"tweetId\"]\n",
    "juul_data.loc[juul_data.tweetId.isin(ids_),\"tweetText\"] = juul_data.loc[juul_data.tweetId.isin(ids_)][\"retweetText\"]\n",
    "juul_data = juul_data.drop([\"hashtags\",\"retweetText\"],axis=1)\n",
    "len(juul_data) ## 1353987 (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673724"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(juul_data) ## 1353987\n",
    "len(juul_data.userID.unique()) ## 673724  reg users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @ returns the data in that year\n",
    "def get_year_data(year):\n",
    "    print(\"year\",year)\n",
    "    users_ = list(first_data[\"userID\"].loc[ \n",
    "        ((first_data.juul_first.dt.year <= year)  & (first_data.juul_first.dt.year > (year-1)))\n",
    "        & ((first_data.weed_first.dt.year == (year + 1)) | (pd.isnull(first_data.weed_first))) ## weed data after 2015\n",
    "                ]) # users who will change after september\n",
    "\n",
    "    poly_turn = list(first_data[\"userID\"].loc[\n",
    "        (first_data.juul_first.dt.year <= year)  & \n",
    "        ((first_data.weed_first.dt.year == (year +1 )))]) ## for labelling based on the next year\n",
    "\n",
    "\n",
    "    print(\"users that will change\",len(poly_turn))\n",
    "    print(\"total users\",len(users_))\n",
    "\n",
    "    ## getting the input data\n",
    "    data_ = juul_data.loc[juul_data.userID.isin(users_)]\n",
    "    print(\"length of data\",len(data_))\n",
    "\n",
    "    ## get label - they reamain same for this task as the no of users, we only change the tweets data\n",
    "    users_ = data_.userID.unique()\n",
    "    users_lbl = pd.DataFrame(users_,columns=[\"userID\"])\n",
    "    users_lbl[\"label\"] = 0  ## initialize\n",
    "    users_lbl.loc[users_lbl.userID.isin(poly_turn) , \"label\"] = 1\n",
    "    len(users_lbl.loc[users_lbl.label == 1])  ## sanity check\n",
    "    return ((year,data_,users_lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/drew_william2345/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/drew_william2345/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## HELPER FUCNTIONS\n",
    "# setup env\n",
    "import nltk\n",
    "from numpy import zeros\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "def get_tokens(sentence):\n",
    "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
    "    tokens = tweet_tknzr.tokenize(sentence)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1)] ## remove punctuations\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return (tokens)\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "def get_max_length(df):\n",
    "    ## max_length\n",
    "    lengths = df[\"tweetText\"].progress_apply(get_length)\n",
    "    max_len = int(lengths.quantile(0.95))\n",
    "    return (max_len)\n",
    "\n",
    "def get_length(s):\n",
    "    a = list(s.split())\n",
    "    return(len(a))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'(https?://\\S+)', \"\", text) ## remove url\n",
    "    text = re.sub(r'(\\@\\w+)', \"author\",text)   ## remove @ mentions with author\n",
    "    text = re.sub(r'(@)', \"\",text)             ## remove @ symbols\n",
    "    text = re.sub(r'(author)',\"\",text)         ## remove author\n",
    "    text = re.sub(r'(#)', \"\",text)             ## removing the hashtags signal \n",
    "    text = re.sub(r'(RT )', \"\",text)         ## remove the retweet info as they dont convey any information\n",
    "    text = re.sub(r'(^:)',\"\",text)\n",
    "    text = text.rstrip() \n",
    "    text = text.lstrip()\n",
    "    return(text)\n",
    "\n",
    "## returns the emnbedding matrix for the lstm model\n",
    "def get_embedding_matrix(vocab_size,dimension,embedding_file,keras_tkzr):\n",
    "    word2vec = get_word2vec(embedding_file)\n",
    "    from numpy import zeros\n",
    "    embedding_matrix = zeros((vocab_size, dimension))\n",
    "    for word, i in keras_tkzr.word_index.items():\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def get_baseline_scores(X_train,X_test,Y_train,Y_test):\n",
    "    \n",
    "    print(\"training the models\")\n",
    "    print(\"svm\")\n",
    "    svm = LinearSVC(C=1,verbose=1)\n",
    "    svm.fit(X_train,Y_train)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    svm_score = precision_recall_fscore_support(Y_test,svm_pred,average=None)[2]  # return the f-score\n",
    "    svm_f1 =  cross_val_score(svm, X_test, Y_test, cv=5,scoring='f1_macro').mean()\n",
    "    print('svm cross val score mean',svm_f1,'\\n')\n",
    "    \n",
    "    print(\"random_forest\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "    rf.fit(X_train,Y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_score = precision_recall_fscore_support(Y_test,rf_pred,average=None)[2]\n",
    "    rf_f1  = cross_val_score(rf, X_test, Y_test, cv=5,scoring='f1_macro').mean()\n",
    "    print('rf cross val score mean',rf_f1,'\\n')\n",
    "    \n",
    "    print(\"xgBoost\")\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_train, Y_train)\n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    xgb_score = precision_recall_fscore_support(Y_test,xgb_pred,average=None)[2]\n",
    "    xgb_f1 =  cross_val_score(xgb, X_test, Y_test, cv=5,scoring='f1_macro').mean()\n",
    "    print('xgb corss val score mean',xgb_f1,'\\n')\n",
    "    \n",
    "    y_pred = [1 for x in range(len(Y_test))]\n",
    "    print('  Classification Report:\\n',classification_report(Y_test,y_pred),'\\n')\n",
    "    maj_score = precision_recall_fscore_support(Y_test,y_pred,average=None)[2]\n",
    "    \n",
    "    models = {0:[svm_pred,svm,\"svm\"], 1:[rf_pred,rf,\"rf\"] ,2:[xgb_pred,xgb,\"xgb\"]}\n",
    "    model_idx = np.argmax([svm_f1,rf_f1,xgb_f1])  ## get the best performing model\n",
    "    \n",
    "    print(\"selecting the best model\",models[model_idx][2])\n",
    "\n",
    "    print(\"job finished\")\n",
    "    all_scores = {\n",
    "        'svm': [svm,svm_score],\n",
    "        'rf' : [rf,rf_score],\n",
    "        'xg_boost': [xgb,xgb_score],\n",
    "        'maj': [maj_score],\n",
    "    }\n",
    "    return (all_scores,models[model_idx])\n",
    "\n",
    "def prepare_data(input_data,users_labelled):\n",
    "    ## preapring the user data\n",
    "    user_data = input_data.groupby(by=\"userID\").agg({  'tweetText' : 'count',\n",
    "                                                       'followersCount': 'first',\n",
    "                                                       'friendsCount' : 'first',\n",
    "                                                       'statusesCount' : 'first',\n",
    "                                                       'favourites_count' : 'first',\n",
    "                                                       'listedCount' : 'first',\n",
    "                                                      })\n",
    "    user_data = user_data.rename(columns={'tweetText': 'tweetCount'})\n",
    "    \n",
    "    # preapring text\n",
    "    tweet_data = input_data.groupby(by=\"userID\")[\"tweetText\"].apply(lambda x: \"%s\" % ' '.join(x)).reset_index()\n",
    "    ## cleaning the text\n",
    "    tweet_data[\"tweetText\"] = tweet_data[\"tweetText\"].progress_apply(clean_text)\n",
    "    tweet_data[\"tweetText\"] = tweet_data[\"tweetText\"].progress_apply(get_tokens).str.join(\" \")\n",
    "    \n",
    "    ## merging the text and user data\n",
    "    final_data = user_data.join(tweet_data.set_index(\"userID\"),on=\"userID\",how=\"inner\").reset_index()\n",
    "    final_data = final_data.fillna(0)   \n",
    "    \n",
    "    ## extract the labels\n",
    "    y = list(final_data.join(users_labelled.set_index(\"userID\"),on=\"userID\",how=\"inner\")[\"label\"]) \n",
    "    print(\"downsampling\")\n",
    "    \n",
    "    ## downsampling based on userIDS\n",
    "    userIDs = np.array(list(final_data.userID)).reshape(-1,1)\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    rus.fit(userIDs,y)\n",
    "    userIDs , y_sam = rus.fit_sample(userIDs,y)\n",
    "    input_data = (final_data.loc[final_data.userID.isin(userIDs.flatten())])\n",
    "    print(\"downsampled data length\",len(input_data))\n",
    "    \n",
    "    print(\"train-test split\")\n",
    "    train_data,test_data,Y_train,Y_test  = train_test_split(input_data, y_sam,test_size =0.20,random_state= 4,shuffle=True ,\n",
    "                                                          stratify= y_sam)\n",
    "    return(train_data,test_data,Y_train,Y_test)\n",
    "\n",
    "## return user fatures\n",
    "def prepare_user_features(input_):\n",
    "    user_data = input_[[\"followersCount\",\"friendsCount\",\"statusesCount\"\n",
    "                          ,\"favourites_count\",\"listedCount\",\"tweetCount\",]]\n",
    "    ## followerss/ friends ration\n",
    "    user_data[\"ff_ratio\"] = user_data[\"followersCount\"] / user_data[\"friendsCount\"]\n",
    "\n",
    "    ## using log of each of the columns\n",
    "    user_data[[\"followersCount\",\"friendsCount\"\n",
    "          ,\"statusesCount\",\"favourites_count\",\n",
    "          \"listedCount\"]] = np.log(user_data[[\"followersCount\",\"friendsCount\",\"statusesCount\",\"favourites_count\",\n",
    "                                        \"listedCount\"]])\n",
    "    \n",
    "    user_data[\"unigrams\"] = list(input_[\"tweetText\"].apply(get_length))\n",
    "\n",
    "    ## replace the na and inf values \n",
    "    user_data = user_data.replace([np.inf, -np.inf], np.nan)\n",
    "    user_data.replace(np.nan,0)\n",
    "    user_data = user_data.fillna(0)\n",
    "\n",
    "    ## normalizing the values\n",
    "    user_data = (user_data - user_data.min())/(user_data.max()-user_data.min())\n",
    "    user_data = user_data.replace([np.inf, -np.inf], np.nan)\n",
    "    user_data = user_data.fillna(0)\n",
    "\n",
    "    X = user_data.values\n",
    "    return (X,user_data)\n",
    "\n",
    "def run_user_features(train_data,test_data,Y_train,Y_test):\n",
    "    X_train,_ = prepare_user_features(train_data)\n",
    "    X_test,_ = prepare_user_features(test_data)\n",
    "    \n",
    "    scores,best_model = get_baseline_scores(X_train,X_test,Y_train,Y_test)\n",
    "    return(scores,best_model[0],best_model[1])\n",
    "\n",
    "##models\n",
    "\n",
    "## @ return a trained svm model\n",
    "def svm_wrapper(X_train,Y_train):\n",
    "    svm = LinearSVC(C=1, verbose=1)\n",
    "    svm.fit(X_train, Y_train)\n",
    "    return svm\n",
    "\n",
    "def training_plot(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def run_lstm(train_data,test_data,Y_train,Y_test,dimension,epoch,metrics,weight=None):\n",
    "    scores = []\n",
    "    ## print winodow , max_len for analysis purpose\n",
    "    max_len = get_max_length(train_data)\n",
    "    if max_len > 60:\n",
    "        max_len = 60\n",
    "    print(\"max_length\",max_len)\n",
    "\n",
    "    ## prepare the tokenizer\n",
    "    print(\"preparing the tokenizer\")\n",
    "    keras_tkzr = keras_Tokenizer()\n",
    "    keras_tkzr.fit_on_texts(train_data[\"tweetText\"])\n",
    "    vocab_size = len(keras_tkzr.word_index) + 1\n",
    "    print(\"vocalb\",vocab_size)\n",
    "\n",
    "    ## embedding matrix\n",
    "    print(\"creating glove embeddign matrix\")\n",
    "    embedding_matrix = get_embedding_matrix(vocab_size,dimension,embedding_file,keras_tkzr) ## tokenizer contains the vocalb info\n",
    "\n",
    "    ## encoding the docs\n",
    "    print(\"encoding the data\")\n",
    "    encoded_docs = keras_tkzr.texts_to_sequences(train_data[\"tweetText\"])\n",
    "    X_train = (pad_sequences(encoded_docs, maxlen=max_len, padding='post'))\n",
    "\n",
    "    ## encoding the test data\n",
    "    encoded_docs = keras_tkzr.texts_to_sequences(test_data[\"tweetText\"])\n",
    "    X_test = (pad_sequences(encoded_docs, maxlen=max_len, padding='post'))\n",
    "\n",
    "    print(\"X-train\",X_train.shape)\n",
    "    print(\"X-test\",X_test.shape)\n",
    "\n",
    "    ## getting the user features\n",
    "    X_train_user,_ = prepare_user_features(train_data)\n",
    "    X_test_user,_ = prepare_user_features(test_data)\n",
    "    \n",
    "    user_feat_len  = (X_train_user.shape[1])\n",
    "    print(\"creating lstm model\")\n",
    "    model = create_model(max_len,user_feat_len,vocab_size,dimension,embedding_matrix)\n",
    "\n",
    "    print(\"training the model with balance dataset\")\n",
    "    history = model.fit([X_train,X_train_user],Y_train,validation_split=0.25 , nb_epoch = epoch, \n",
    "                        verbose = 1,batch_size=32,class_weight= None,)\n",
    "\n",
    "    ##plotting trainin validation - no point as we dont want ot look at accuarcy\n",
    "    training_plot(history)\n",
    "\n",
    "\n",
    "    scores = get_cross_val_score(train_data,Y_train,n_splits=5,nb_epoch=epoch)\n",
    "\n",
    "    print(\"generating classfication report\")\n",
    "    loss, accuracy = model.evaluate([X_test,X_test_user], Y_test, verbose=2)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    ## lstm model\n",
    "    temp = model.predict([X_test,X_test_user])\n",
    "    y_pred = [np.argmax(value) for value in temp]  ## sigmoid\n",
    "    print('  Classification Report:\\n',classification_report(Y_test,y_pred),'\\n')\n",
    "\n",
    "    print(\"lstm cross val score \",np.array(scores).mean() )\n",
    "\n",
    "    print(\"job finished\")\n",
    "    return (scores,y_pred,model,keras_tkzr,max_len)\n",
    "\n",
    "## @ return a trained svm model on text features for LR\n",
    "def run_text_features(train_data,test_data,Y_train,Y_test):\n",
    "    tf_idf = TfidfVectorizer(sublinear_tf=True)\n",
    "    tf_idf.fit(train_data[\"tweetText\"])  ## fit on train data\n",
    "    \n",
    "     ## transform train and test data\n",
    "    X_test = tf_idf.transform(test_data[\"tweetText\"])\n",
    "    X_train = tf_idf.transform(train_data[\"tweetText\"])\n",
    "    \n",
    "    \n",
    "    ## reduce the dimesionality\n",
    "    svd = TruncatedSVD(n_components=500, n_iter=7, random_state=42)\n",
    "    svd.fit(X_train)  \n",
    "    X_train = svd.transform(X_train)\n",
    "    X_test = svd.transform(X_test)\n",
    "    \n",
    "    scores,best_model = get_baseline_scores(X_train,X_test,Y_train,Y_test)\n",
    "    return (scores,best_model[0],best_model[1],tf_idf,svd)\n",
    "\n",
    "def cal_text_pred(test_data,Y_test,model,tf_idf,svd):\n",
    "    X_test = tf_idf.transform(test_data[\"tweetText\"])\n",
    "    X_test = svd.transform(X_test) ## reduce the dimensionality\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def cal_lstm_pred(test_data,Y_test,model,keras_tkzr,max_len):\n",
    "     ## encoding the test data\n",
    "    encoded_docs = keras_tkzr.texts_to_sequences(test_data[\"tweetText\"])\n",
    "    X_test = (pad_sequences(encoded_docs, maxlen=max_len, padding='post'))\n",
    "    X_test_user,_ = prepare_user_features(test_data)\n",
    "    ## calculate the model predictions\n",
    "    temp = model.predict([X_test,X_test_user])\n",
    "    y_pred = [np.argmax(value) for value in temp]  ## sigmoid\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def cal_user_pred(test_data,Y_test,model):\n",
    "    X_test,_ = prepare_user_features(test_data)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# create the word2vec dict from the dictionary\n",
    "def get_word2vec(file_path):\n",
    "    file = open(file_path, \"r\")\n",
    "    if (file):\n",
    "        word2vec = dict()\n",
    "#         split = file.read().splitlines()\n",
    "        for line in file:\n",
    "            split_line = line.split(' ')\n",
    "            key = split_line[0] # the first word is the key\n",
    "            value = np.array([float(val) for val in split_line[1:]])\n",
    "            word2vec[key] = value\n",
    "        return (word2vec)\n",
    "    else:\n",
    "        print(\"invalid fiel path\")\n",
    "        \n",
    "from keras.callbacks import Callback\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_predict = np.array([np.argmax(value) for value in val_predict])\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print (\"— val_f1: %f — val_precision: %f — val_recall %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        print('  Classification Report:\\n',classification_report(val_targ,val_predict),'\\n')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "## handle two different inputs and then concatenate them (user and text features)\n",
    "## input = [words_in,user_in]\n",
    "def create_model(max_len,user_feature_len,vocalb_size,dimension,embedding_matrix):\n",
    "    ## handle text features..\n",
    "    words_in = Input(shape=(max_len,))\n",
    "    emb_word = Embedding(vocab_size,dimension,weights=[embedding_matrix],input_length=max_len)(words_in) \n",
    "    lstm_word =  Bidirectional (LSTM (100,return_sequences=False,dropout=0.50,kernel_regularizer=regularizers.l2(0.01)),merge_mode='concat')(emb_word)\n",
    "    lstm_word = Dense(user_feature_len,activation='relu')(lstm_word)\n",
    "    \n",
    "    ## takes the user features as input\n",
    "    user_input = Input(shape=(user_feature_len,))\n",
    "    \n",
    "    ## concatenate both of the features\n",
    "    modelR = concatenate([lstm_word, user_input])\n",
    "    # modelR = SpatialDropout1D(0.1)(modelR)\n",
    "    output = Dense(2,activation='softmax')(modelR)\n",
    "    model = Model([words_in,user_input],output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return cross_val mean score for each \n",
    "def get_cross_val_score(train_data,Y_train,n_splits,nb_epoch):\n",
    "    scores = []\n",
    "    train_ids = list(train_data.index)\n",
    "    kFold = StratifiedKFold(n_splits=n_splits)\n",
    "    for train, test in kFold.split(train_ids,Y_train):\n",
    "        X_train_user,_ = prepare_user_features(train_data.loc[train])\n",
    "        X_test_user,_ = prepare_user_features(train_data.loc[test])\n",
    "\n",
    "        encoded_docs = keras_tkzr.texts_to_sequences(train_data.loc[train][\"tweetText\"])\n",
    "        X_train = (pad_sequences(encoded_docs, maxlen=max_len, padding='post'))\n",
    "        encoded_docs = keras_tkzr.texts_to_sequences(train_data.loc[test][\"tweetText\"])\n",
    "        X_test = (pad_sequences(encoded_docs, maxlen=max_len, padding='post'))\n",
    "\n",
    "        history = model.fit([X_train,X_train_user],Y_train[train],validation_split=0.25 , nb_epoch = epoch, \n",
    "                        verbose = 1,batch_size=32,class_weight= None,)\n",
    "        training_plot(history)\n",
    "\n",
    "        ## prediction\n",
    "        temp = model.predict([X_test,X_test_user])\n",
    "        y_pred = [np.argmax(value) for value in temp]  ## sigmoid\n",
    "        f1 = precision_recall_fscore_support(Y_train[test],y_pred,average=None)[2]\n",
    "        print(f1)\n",
    "        print('  Classification Report:\\n',classification_report(Y_train[test],y_pred),'\\n')\n",
    "        scores.append(f1)\n",
    "    score1 = np.mean([ele[0] for ele in scores])\n",
    "    score2 = np.mean([ele[1] for ele in scores])\n",
    "    return (score1,score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 2015\n",
      "users that will change 97\n",
      "total users 1049\n",
      "length of data 2944\n",
      "year 2016\n",
      "users that will change 313\n",
      "total users 1636\n",
      "length of data 6475\n",
      "year 2017\n",
      "users that will change 9525\n",
      "total users 40236\n",
      "length of data 111735\n"
     ]
    }
   ],
   "source": [
    "## gathering data in different range of intervals\n",
    "bucket_data = []\n",
    "bucket_data.append(get_year_data(2015))\n",
    "bucket_data.append(get_year_data(2016))\n",
    "bucket_data.append(get_year_data(2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1049 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1049/1049 [00:00<00:00, 49708.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1049 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "preparing the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 251/1049 [00:00<00:00, 2506.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 557/1049 [00:00<00:00, 2618.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 780/1049 [00:00<00:00, 2487.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▌| 1005/1049 [00:00<00:00, 2411.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1049/1049 [00:00<00:00, 2429.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/155 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 155/155 [00:00<00:00, 51682.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampling\n",
      "downsampled data length 194\n",
      "train-test split\n",
      "runnning classfier to get user features\n",
      "max_length 60\n",
      "preparing the tokenizer\n",
      "vocalb 2073\n",
      "creating glove embeddign matrix\n"
     ]
    }
   ],
   "source": [
    "## running by year\n",
    "dimension = 100\n",
    "for year,data,users_labelled in bucket_data:\n",
    "    print(\"************\")\n",
    "    print(\"preparing the data\")\n",
    "    train_data,test_data,Y_train,Y_test = prepare_data(data,users_labelled)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    print(\"runnning classfier to get user features\")\n",
    "    \n",
    "    scores,y_pred,lstm_model,keras_tkzr,max_len = run_lstm(train_data,test_data,Y_train,Y_test,\n",
    "                                                      dimension,epoch,metrics,weight=None)\n",
    "    \n",
    "    ## getting predicting on test data\n",
    "    y_pred = cal_lstm_pred(test_data,Y_test,lstm_model,keras_tkzr,max_len)\n",
    "    print('  Classification Report test data:\\n',classification_report(Y_test,y_pred),'\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
