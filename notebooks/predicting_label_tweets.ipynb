{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import git\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Average\n",
    "from keras_contrib.layers import CRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the git repo\n",
    "def get_git_root(path):\n",
    "\tgit_repo = git.Repo(path, search_parent_directories=True)\n",
    "\tgit_root = git_repo.git.rev_parse(\"--show-toplevel\")\n",
    "\treturn git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/samarthgoal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/samarthgoal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup env\n",
    "# setup env\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## tokenize the text..\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "def get_tokens(sentence):\n",
    "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
    "    tokens = tknzr.tokenize(sentence)\n",
    "    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1)]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'(https?://\\S+)', \"\", text)\n",
    "    text = re.sub(r'(\\@\\w+)', \"author\",text)\n",
    "    text.rstrip\n",
    "    text.lstrip\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings_dir  = os.path.join(get_git_root(os.getcwd()),\"input\",\"embeddings\")\n",
    "extract_dir = os.path.join(get_git_root(os.getcwd()),\"input\",\"hexagon_extract\")\n",
    "annotatted_dir = \"/home/samarthgoal/misc_data/annotated_data/\"\n",
    "classifier_dir = os.path.join(get_git_root(os.getcwd()),\"models\",\"classifier\")\n",
    "input_dir = os.path.join(get_git_root(os.getcwd()),\"input\")\n",
    "model_dir = os.path.join(get_git_root(os.getcwd()),\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_file = os.listdir(embeddings_dir)[0]\n",
    "annotatted_files = os.listdir(annotatted_dir)\n",
    "hexagon_files = [file for file in os.listdir(extract_dir) if file.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing the  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweetText', 'label'], dtype='object')\n",
      "Index(['tweetText', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## extracting the annotated data\n",
    "df_annotation = pd.DataFrame()\n",
    "for file in annotatted_files:\n",
    "    df_ = pd.read_excel(os.path.join(annotatted_dir,file))\n",
    "    columns = (df_.columns)\n",
    "    df_ = df_.rename(columns={columns[1]: \"label\", columns[0]: \"tweetText\"})\n",
    "    print(df_.columns)\n",
    "    frames = [df_,df_annotation]\n",
    "    df_annotation = pd.concat(frames,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = (df_annotation.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_annotation = df_annotation.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "def get_tokens(sentence):\n",
    "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
    "    tokens = tknzr.tokenize(sentence)\n",
    "    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1 and token.isalpha())]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the word2vec dict from the dictionary\n",
    "def get_word2vec(file_path):\n",
    "    file = open(file_path, \"r\")\n",
    "    if (file):\n",
    "        word2vec = dict()\n",
    "        split = file.read().splitlines()\n",
    "        for line in split:\n",
    "            key = line.split(' ',1)[0] # the first word is the key\n",
    "            value = np.array([float(val) for val in line.split(' ')[1:]])\n",
    "            word2vec[key] = value\n",
    "        return (word2vec)\n",
    "    else:\n",
    "        print(\"invalid fiel path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "\tdef __init__(self, word2vec):\n",
    "\t\tself.word2vec = word2vec\n",
    "\t\t# if a text is empty we should return a vector of zeros\n",
    "\t\t# with the same dimensionality as all the other vectors\n",
    "\t\tself.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\treturn np.array([\n",
    "\t\t\tnp.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "\t\t\t\t\tor [np.zeros(self.dim)], axis=0)\n",
    "\t\t\tfor words in tqdm(X)\n",
    "\t\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## tokenize the sentences..\n",
    "X = list(df_annotation['tweetText'].apply(get_tokens))\n",
    "y = (list(df_annotation['label']))\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_len = len(max(token_list, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2843"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the input: pad and fit\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(token_list)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = get_word2vec(os.path.join(embeddings_dir,embedding_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = vocab_size\n",
    "encoded_docs = t.texts_to_sequences(token_list)\n",
    "# pad documents to a max length of 4 words\n",
    "max_len = max_len\n",
    "max_len = 60\n",
    "X = pad_sequences(encoded_docs, maxlen=max_len, padding='post')\n",
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## train test split\n",
    "X_train,X_test,Y_train,Y_test  = train_test_split(X, y,test_size =0.20,random_state= 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get the embedding matrix weights:\n",
    "from numpy import zeros\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## pass to the bi-lstm model\n",
    "max_len= 60\n",
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(vocab_size,100,weights=[embedding_matrix],input_length=max_len)(input)\n",
    "model =  Bidirectional (LSTM (100,return_sequences=True,dropout=0.50),merge_mode='concat')(model)\n",
    "model = TimeDistributed(Dense(100,activation='relu'))(model)\n",
    "model = Flatten()(model)\n",
    "model = Dense(100,activation='relu')(model)\n",
    "output = Dense(3,activation='softmax')(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(input,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_34 (Embedding)     (None, 60, 100)           284300    \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 60, 200)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 60, 100)           20100     \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 6000)              0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 100)               600100    \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,065,603\n",
      "Trainable params: 1,065,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarthgoal/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 311 samples, validate on 104 samples\n",
      "Epoch 1/10\n",
      " - 14s - loss: 1.0601 - acc: 0.4405 - val_loss: 1.0292 - val_acc: 0.5577\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.9096 - acc: 0.6045 - val_loss: 0.9163 - val_acc: 0.5865\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.7607 - acc: 0.6656 - val_loss: 0.9105 - val_acc: 0.6250\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.6692 - acc: 0.7235 - val_loss: 0.9828 - val_acc: 0.6250\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.5839 - acc: 0.7685 - val_loss: 0.8812 - val_acc: 0.6538\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.4938 - acc: 0.8264 - val_loss: 0.9705 - val_acc: 0.5962\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.4833 - acc: 0.8071 - val_loss: 1.0160 - val_acc: 0.6635\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.3975 - acc: 0.8392 - val_loss: 1.0637 - val_acc: 0.5962\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.3161 - acc: 0.8907 - val_loss: 1.0680 - val_acc: 0.6731\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.2768 - acc: 0.8939 - val_loss: 1.1217 - val_acc: 0.6346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d6726ca20>"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pass the training data\n",
    "model.fit(X_train,Y_train,validation_split=0.25, nb_epoch = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.000000\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85        39\n",
      "           1       0.70      0.57      0.63        28\n",
      "           2       0.70      0.76      0.73        37\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       104\n",
      "   macro avg       0.74      0.73      0.73       104\n",
      "weighted avg       0.75      0.75      0.75       104\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.array([np.argmax(pred) for pred in Y_pred])\n",
    "print('  Classification Report:\\n',classification_report(Y_test,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.884615\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "##with max_len =55\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89        39\n",
      "           1       0.66      0.68      0.67        28\n",
      "           2       0.81      0.68      0.74        37\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       104\n",
      "   macro avg       0.77      0.77      0.76       104\n",
      "weighted avg       0.78      0.78      0.78       104\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##with max_len =55\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.array([np.argmax(pred) for pred in Y_pred])\n",
    "print('  Classification Report:\\n',classification_report(Y_test,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## dump the classifier(neural model)\n",
    "import pickle\n",
    "filename = os.path.join(classifier_dir,\"lstm_model.pkl\")\n",
    "with open(filename,\"wb\") as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the baseline with simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [00:00<00:00, 21238.54it/s]\n"
     ]
    }
   ],
   "source": [
    "vect = MeanEmbeddingVectorizer(word2vec)\n",
    "X = vect.transform(token_list)\n",
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.25,random_state =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "rf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etree = ExtraTreesClassifier(n_estimators=200)\n",
    "etree.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_wrapper(X_train,Y_train):\n",
    "    param_grid = [\n",
    "    {'C': [1, 10], 'kernel': ['linear']},\n",
    "    {'C': [1, 10], 'gamma': [0.1,0.01], 'kernel': ['rbf']},]\n",
    "    svm1 = GridSearchCV(SVC(),param_grid)\n",
    "    svm1.fit(X_train, Y_train)\n",
    "    return(svm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarthgoal/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sv = svm_wrapper(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.78      0.67        49\n",
      "           1       0.27      0.08      0.13        36\n",
      "           2       0.56      0.69      0.62        45\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       130\n",
      "   macro avg       0.48      0.52      0.47       130\n",
      "weighted avg       0.49      0.55      0.50       130\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5538461538461539"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = rf.predict(X_test)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73        49\n",
      "           1       0.56      0.39      0.46        36\n",
      "           2       0.60      0.69      0.64        45\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       130\n",
      "   macro avg       0.62      0.61      0.61       130\n",
      "weighted avg       0.62      0.63      0.62       130\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6307692307692307"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = sv.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72        49\n",
      "           1       0.46      0.31      0.37        36\n",
      "           2       0.60      0.73      0.66        45\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       130\n",
      "   macro avg       0.59      0.59      0.58       130\n",
      "weighted avg       0.60      0.62      0.60       130\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6153846153846154"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = etree.predict(X_test)\n",
    "print('  Classification Report:\\n',classification_report(Y_test,Y_pred),'\\n')\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarthgoal/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.53448276, 0.53179191, 0.61046512])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(etree,X,y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## lets predict with the model\n",
    "## create a big file of constituents file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(os.path.join(extract_dir,hexagon_files[0]),lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4917"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retweets\n",
    "len([ele for ele in list(df_tweets[\"retweetText\"]) if not pd.isnull(ele)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8698"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarthgoal/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## merge all of the files\n",
    "df_final = pd.DataFrame()\n",
    "for file in hexagon_files:\n",
    "    filename = os.path.join(extarct_dir,file)\n",
    "    df = pd.read_csv(filename,lineterminator=\"\\n\")\n",
    "    frames = [df,df_final]\n",
    "    df_final = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_final = df_final.loc[:, ~df_final.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4857"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## total length of df_final\n",
    "len(df_final.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of retweets: 57.50130308967054\n"
     ]
    }
   ],
   "source": [
    "## retweets\n",
    "retweets_count = len([ele for ele in list(df_final[\"retweetText\"]) if not pd.isnull(ele)])\n",
    "## no of retweets : 63984\n",
    "print(\"% of retweets:\", (retweets_count / len(df_final))*100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## removing the retweets -- we gonna classify the original tweets based on bilstm model\n",
    "df_temp = df_final.loc[df_final['retweetText'].isnull() == True]  # removing the rewteets\n",
    "df_original = df_temp.loc[~df_temp['tweetText'].str.startswith('RT', na=False)]   # removing the text that contains retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_test = list(df_original['tweetText'].apply(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45596"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(df_original['tweetText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_list = list(df_final['tweetText'].apply(get_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111274"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_len = 60\n",
    "encoded_docs = t.texts_to_sequences(token_list)\n",
    "X_new = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45596"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_new = [np.argmax(ele) for ele in model.predict(X_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarthgoal/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_original['label'] = Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27255"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_original.loc[df_original.label == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12323"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## news campaign\n",
    "len(df_original.loc[df_original.label == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_input_pred  = df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2878"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_input_pred.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting the pattern for juul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total users =  2878\n",
      "no of poly users =  0\n",
      "no of mono users =  2878\n",
      "*** starting with the poly sub type users\n",
      "Poly type users calculated\n",
      "total users = 2878\n",
      "****************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-743-59bfbbc4925d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"total users =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"****************\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"% of pol1 users = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_user1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"% of pol2 users = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_user2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "## predicting the mono and poly users\n",
    "weed_words = pickle.load(open(os.path.join(model_dir, \"weed_words.pkl\"), \"rb\"))\n",
    "weed_words = [(\" \" + word + \" \") for word in weed_words]\n",
    "pattern_weed = \"|\".join(weed_words)\n",
    "pattern_juul = 'juul'\n",
    "print(\"extracting the pattern for juul\")\n",
    "df_tweet_weeds = df_input_pred[df_input_pred['tweetText'].str.contains(pattern_weed, case=False)]\n",
    "index = df_tweet_weeds.index  # the file after filtering have the index contained within\n",
    "df_weeds = df_input_pred.loc[index]\n",
    "poly_users = list(set(list(df_weeds.loc[df_weeds.label == 3]['userID'])))\n",
    "total_users = list(df_input_pred.userID.unique())\n",
    "poly_length = len(poly_users)\n",
    "total_users_length = len(total_users)\n",
    "mono_length = total_users_length - poly_length\n",
    "print(\"total users = \", total_users_length)\n",
    "print(\"no of poly users = \", poly_length)\n",
    "print(\"no of mono users = \", mono_length)\n",
    "# print(\"% of poly users is \", poly_length / total_users_length)\n",
    "# print(\"% of mono users is \", mono_length / total_users_length)\n",
    "print(\"*** starting with the poly sub type users\")\n",
    "poly_user1 = list()\n",
    "poly_user2 = list()\n",
    "poly_user3 = list()\n",
    "poly_und = list()\n",
    "total_users = list(df_input_pred.userID.unique())\n",
    "for user in tqdm(poly_users):\n",
    "    user_tweets = df_input_pred.loc[df_input_pred.userID == user]\n",
    "    user_tweets.sort_values(by='tweetCreatedAt', ascending=True,\n",
    "                            inplace=True)  # sort by tweet created at\n",
    "    juul_tweets = user_tweets[user_tweets['tweetText'].str.contains(pattern_juul, case=False)]\n",
    "    juul_tweets.reset_index(drop=True, inplace=True)\n",
    "    if (len(juul_tweets) > 0):\n",
    "        time_j = pd.to_datetime(\n",
    "            juul_tweets.head(1)['tweetCreatedAt'].values[0])  # getting the tweet with\n",
    "    else:\n",
    "        time_j = None\n",
    "    weed_tweets = user_tweets[user_tweets['tweetText'].str.contains(pattern_weed, case=False)]\n",
    "    if (len(weed_tweets) > 0):\n",
    "        weed_tweets_user = weed_tweets[weed_tweets.label == 3]\n",
    "        if (len(weed_tweets_user) > 0):\n",
    "            times_w = pd.to_datetime(list(weed_tweets_user[\n",
    "                                              'tweetCreatedAt']))\n",
    "    else:\n",
    "        times_w = None\n",
    "    if (time_j != None and times_w is not None):\n",
    "        pos = list(times_w).index(util.nearest((times_w), time_j))\n",
    "        if (pos >= len(times_w)):\n",
    "            poly_user1.append(user)\n",
    "        elif (pos == 0):\n",
    "            poly_user2.append(user)\n",
    "        else:\n",
    "            poly_user3.append(user)\n",
    "    else:\n",
    "        poly_und.append(user)\n",
    "print(\"Poly type users calculated\")\n",
    "print(\"total users =\", len(total_users))\n",
    "print(\"****************\\n\")\n",
    "print(\"% of pol1 users = \", len(poly_user1) / len(poly_users))\n",
    "print(\"\\n\")\n",
    "print(\"% of pol2 users = \", len(poly_user2) / len(poly_users))\n",
    "print(\"\\n\")\n",
    "print(\"% of pol3 users = \", len(poly_user3) / len(poly_users))\n",
    "print(\"\\n\")\n",
    "print(\"% of undefined users = \", len(poly_und) / len(poly_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## predicted data\n",
    "filepath = os.path.join(input_dir,\"labelled_data\")\n",
    "df_original.to_csv(os.path.join(filepath,\"tweets_predicted.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## annnotated data\n",
    "filepath = os.path.join(input_dir,\"annotated_data\")\n",
    "df_annotation.to_csv(os.path.join(filepath,\"annotated_hex.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
